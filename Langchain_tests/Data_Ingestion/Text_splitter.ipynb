{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b50cbf0",
   "metadata": {},
   "source": [
    "# Text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe7635e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading a PDF file\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"attention.pdf\")\n",
    "docs = loader.load()\n",
    "len(docs)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14aafaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b316ac6c",
   "metadata": {},
   "source": [
    "# How to recursively split text by character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "144de28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='University of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='based solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='architectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='tion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='block, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='used successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='language modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='wise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Decoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='masking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Attention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='much faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='extremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='i ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='and queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='FFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='Convolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='The third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='the maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='path length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='from our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='target tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='warmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='Deep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='the competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='architectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='big 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='models have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='for the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Zhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='English-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='such as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='arXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='arXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='model. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='layer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='arXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='but\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='but\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to recursively split text by character\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "final_docs = text_splitter.split_documents(docs)\n",
    "final_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b447c9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n",
      "page_content='University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(final_docs[0])\n",
    "print(final_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1437b9ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='Robert Francis Prevost (pictured) is elected as Pope Leo XIV, becoming the first Catholic pope born in the United States.\\nFriedrich Merz is elected Chancellor of Germany and sworn in alongside his coalition government.\\nZhao Xintong defeats Mark Williams to win the World Snooker Championship.\\nIn horse racing, Sovereignty, ridden by Junior Alvarado, wins the Kentucky Derby.\\nThe Australian Labor Party increases its majority in the federal election.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Loader\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "docs = loader.load()\n",
    "len(docs)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dea35c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert Francis Prevost (pictured) is elected as Pope Leo XIV, becoming the first Catholic pope born in the United States.\n",
      "Friedrich Merz is elected Chancellor of Germany and sworn in alongside his coalition government.\n",
      "Zhao Xintong defeats Mark Williams to win the World Snooker Championship.\n",
      "In horse racing, Sovereignty, ridden by Junior Alvarado, wins the Kentucky Derby.\n",
      "The Australian Labor Party increases its majority in the federal election.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "with open(\"speech.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    \n",
    "print(text)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7e234c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Foo'}, page_content='Foo'),\n",
       " Document(metadata={'Header 1': 'Foo'}, page_content='Some intro text about Foo.'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section'}, page_content='Bar main section'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section'}, page_content='Some intro text about Bar.'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 1'}, page_content='Bar subsection 1'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 1'}, page_content='Some text about the first subtopic of Bar.'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 2'}, page_content='Bar subsection 2'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Bar main section', 'Header 3': 'Bar subsection 2'}, page_content='Some text about the second subtopic of Bar.'),\n",
       " Document(metadata={'Header 1': 'Foo', 'Header 2': 'Baz'}, page_content='Baz'),\n",
       " Document(metadata={'Header 1': 'Foo'}, page_content='Some text about Baz  \\nSome concluding text about Foo')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "html_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "    <div>\n",
    "        <h1>Foo</h1>\n",
    "        <p>Some intro text about Foo.</p>\n",
    "        <div>\n",
    "            <h2>Bar main section</h2>\n",
    "            <p>Some intro text about Bar.</p>\n",
    "            <h3>Bar subsection 1</h3>\n",
    "            <p>Some text about the first subtopic of Bar.</p>\n",
    "            <h3>Bar subsection 2</h3>\n",
    "            <p>Some text about the second subtopic of Bar.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h2>Baz</h2>\n",
    "            <p>Some text about Baz</p>\n",
    "        </div>\n",
    "        <br>\n",
    "        <p>Some concluding text about Foo</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on=[\n",
    "    (\"h1\",\"Header 1\"),\n",
    "    (\"h2\",\"Header 2\"),\n",
    "    (\"h3\",\"Header 3\")\n",
    "]\n",
    "\n",
    "html_splitter=HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits=html_splitter.split_text(html_string)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "791cebb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='End container NOTE: Script required for drop-down button to work (mirrors).  \\nEnd header wrapper End content End footer  \\nEnd header  \\nEnd navigation End search  \\nStanford Encyclopedia of Philosophy  \\nMenu  \\nBrowse  \\nTable of Contents  \\nWhat\\'s New  \\nRandom Entry  \\nChronological  \\nArchives  \\nAbout  \\nEditorial Information  \\nAbout the SEP  \\nEditorial Board  \\nHow to Cite the SEP  \\nSpecial Characters  \\nAdvanced Tools  \\nContact  \\nSupport SEP  \\nSupport the SEP  \\nPDFs for SEP Friends  \\nMake a Donation  \\nSEPIA for Libraries  \\nBegin article sidebar End article sidebar NOTE: Article content must have two wrapper divs: id=\"article\" and id=\"article-content\" End article NOTE: article banner is outside of the id=\"article\" div. End article-banner  \\nEntry Navigation  \\nEntry Contents  \\nBibliography  \\nAcademic Tools  \\nFriends PDF Preview  \\nAuthor and Citation Info  \\nBack to Top  \\nEnd article-content  \\nBEGIN ARTICLE HTML #aueditable DO NOT MODIFY THIS LINE AND BELOW END ARTICLE HTML  \\nDO NOT MODIFY THIS LINE AND ABOVE'),\n",
       " Document(metadata={'Header 1': 'Kurt Gödel'}, page_content='Kurt Gödel'),\n",
       " Document(metadata={'Header 1': 'Kurt Gödel'}, page_content='First published Tue Feb 13, 2007; substantive revision Fri Dec 11, 2015  \\nKurt Friedrich Gödel (b. 1906, d. 1978) was one of the principal\\nfounders of the modern, metamathematical era in mathematical logic. He\\nis widely known for his Incompleteness Theorems, which are among the\\nhandful of landmark theorems in twentieth century mathematics, but his\\nwork touched every field of mathematical logic, if it was not in most\\ncases their original stimulus. In his philosophical work Gödel\\nformulated and defended mathematical Platonism, the view that\\nmathematics is a descriptive science, or alternatively the view that\\nthe concept of mathematical truth is objective. On the basis of that\\nviewpoint he laid the foundation for the program of conceptual\\nanalysis within set theory (see below). He adhered to Hilbert’s\\n“original rationalistic conception” in mathematics (as he\\ncalled\\n it); and he was prophetic in anticipating and emphasizing the importance\\nof large cardinals in set theory before their importance became\\nclear.  \\n[ ]  \\n1  \\nEntry Contents Entry Contents  \\n1. Biographical Sketch  \\n2. Gödel’s Mathematical Work  \\n2.1 The Completeness Theorem  \\n2.1.1 Introduction  \\n2.1.2 Proof of the Completeness Theorem  \\n2.1.3 An Important Consequence of the Completeness Theorem  \\n2.2 The Incompleteness Theorems  \\n2.2.1 The First Incompleteness Theorem  \\n2.2.2 The proof of the First Incompleteness Theorem  \\n2.2.3 The Second Incompleteness Theorem  \\nSupplementary Document: Did the Incompleteness Theorems Refute Hilbert’s Program?  \\n2.3 Speed-up Theorems  \\n2.4 Gödel’s Work in Set theory  \\n2.4.1 The consistency of the Continuum Hypothesis and the Axiom of Choice  \\n2.4.2 Gödel’s Proof of the Consistency of the Continuum Hypothesis and the Axiom of Choice with the Axioms of Zermelo-Fraenkel Set Theory  \\n2.4.3 Consequences of Consistency  \\n2.4.4 Gödel’s view of the Axiom of Constructibility  \\n2.5 Gödel’s Work in Intuitionistic Logic and Arithmetic  \\n2.5.1 Intuitionistic Propositional Logic is not Finitely-Valued  \\n2.5.2 Classical Arithmetic is Interpretable in Heyting Arithmetic  \\n2.5.3 Intuitionistic Propositional Logic is Interpretable in  \\nS4  \\n2.5.4 Heyting Arithmetic is Interpretable into Computable Functionals of Finite Type.  \\nSupplement Document: Gödel’s Documents  \\n3. Gödel’s Philosophical Views  \\n3.1 Gödel’s Rationalism  \\n3.2 Gödel’s Realism  \\nSupplementary Document: Gödel’s Turn to Phenomenology  \\nSupplementary Document: A Philosophical Argument About the Content of Mathematics  \\nBibliography  \\nPrimary Sources  \\nGödel’s Writings  \\nThe Collected Papers of Kurt Gödel  \\nSelected Works of Kurt Gödel  \\nSecondary Sources  \\nAcademic Tools  \\nOther Internet Resources  \\nRelated Entries  \\n1. Biographical Sketch  \\nKurt Gödel was born on April 28, 1906 in what was then the\\nAustro-Hungarian city of Brünn, and what is now Brno in the Czech\\nRepublic.  \\nGödel’s father Rudolf August was a businessman, and his\\nmother Marianne was a well-educated and cultured woman to whom\\nGödel remained close throughout his life, as witnessed by the\\nlong and wide-ranging correspondence between them. The family was well\\noff, and Gödel’s childhood was an uneventful one, with one\\nimportant exception; namely, from about the age of four Gödel\\nsuffered frequent episodes of poor health, and the health problems he\\nsuffered then as well as others of various kinds were to plague him\\nhis entire life.  \\nHealth problems notwithstanding, Gödel proved to be an exemplary\\nstudent at primary school and later the Gymnasium, excelling\\nespecially in mathematics, languages and religion. Upon his graduation\\nfrom the Gymnasium in Brno in 1924 Gödel enrolled in the\\nUniversity of Vienna, attending lectures on physics, his initial field\\nof interest, lectures on philosophy given by Heinrich Gomperz, and\\nlectures on mathematics. Gödel took a number of physics courses\\nduring his undergraduate years, as witnessed by his university\\ntranscript; this is notable in view of Gödel’s subsequent\\ncontributions to relativity in 1947. Philipp Furtwängler, cousin\\nof the great German conductor Wilhelm Furtwängler, was one of his\\nmathematics professors, and indeed Furtwängler’s course on\\nclass field theory almost tempted Gödel to pursue his studies in\\nthat area. Gödel learned his logic from Rudolph Carnap and from\\nHans Hahn, eventually graduating under Hahn with a Dr.phil. in\\nmathematics in 1929. The main theorem of his dissertation was the\\ncompleteness theorem for first order logic (Gödel\\n 1929).  \\n[ ]  \\n2  \\nGödel’s university years also marked the beginning of his\\nattendance at meetings of the Vienna Circle, a group around Moritz\\nSchlick that quickly became known as “logical\\npositivists,” a term coined by Feigl and Blumberg in their 1931\\n“Logical positivism: A new movement in European\\nphilosophy” (Feigl and Blumberg 1931). Though Gödel was not\\nhimself a logical positivist, those discussions were a crucial\\nformative influence.  \\nThe 1930s were a prodigious decade for Gödel. After publishing\\nhis 1929 dissertation in 1930, he published his groundbreaking\\nincompleteness theorems in 1931, on the basis of which he was granted\\nhis Habilitation in 1932 and a Privatdozentur at the University of\\nVienna in 1933.  \\nAmong his mathematical achievements at the decade’s close is the\\nproof of the consistency of both the Axiom of Choice and\\nCantor’s Continuum Hypothesis with the Zermelo-Fraenkel axioms\\nfor set theory, obtained in 1935 and 1937, respectively. Gödel\\nalso published a number of significant papers on modal and\\nintuitionistic logic and arithmetic during this period, principal\\namong which is his “On intuitionistic arithmetic and number\\ntheory,” (Gödel 1933e), in which he showed that classical\\nfirst order arithmetic is interpretable in Heyting arithmetic by a\\nsimple translation. Other publications of the 1930s include those on\\nthe decision problem for the predicate calculus, on the length of\\nproofs, and on differential and projective geometry.  \\nBy the end of the decade both Gödel’s advisor Hans Hahn and\\nMoritz Schlick had died (the latter was assassinated by an\\nex-student), two events which led to a personal crisis for Gödel.\\nAlso, his appointment at the University, that of Privatdozentur, was\\ncancelled, being replaced by the position “Dozentur neuer\\nOrdnung,” granted to candidates only after they had passed a\\nracial\\n test. Gödel’s three trips the United States during that decade\\ntriggered an investigation. (See Sigmund 2006.) Finally, Gödel\\nwas found fit for military service by the Nazi government in 1939.  \\n[ ]  \\n3  \\nAll of these events were decisive in influencing his decision to leave\\nAustria in 1940, when he and his wife Adele emigrated to the United\\nStates. This long and difficult episode in their life is recounted by\\nJohn Dawson in his biography of Gödel called “Logical\\nDilemmas,” (Dawson 1997) as well as by Solomon Feferman in\\n“Gödel’s Life and Work,” (Feferman 1986) to\\nboth of which the reader is referred.  \\nUpon arrival Gödel took up an appointment as an ordinary member\\nat the Institute for Advanced Study; he would become a permanent\\nmember of the Institute in 1946 and would be granted his professorship\\nin 1953. (Gödel and his wife were granted American citizenship in\\nApril 1948.) He would remain at the Institute until his retirement in\\n1976. The Gödels never returned to Europe.  \\nGödel’s early years at the Institute were notable for his\\nclose friendship with his daily walking partner Albert Einstein, as\\nwell as for his turn to philosophy of mathematics, a field on which\\nGödel began to concentrate almost exclusively from about 1943.\\nThe initial period of his subsequent lifelong involvement with\\nphilosophy was a fruitful one (in terms of publications): in 1944 he\\npublished his first philosophical paper, entitled “On\\nRussell’s Mathematical Logic” (Gödel 1944), and in\\n1947 he published his second, entitled “What is Cantor’s\\nContinuum Hypothesis?” (Gödel 1947). In 1949 he published\\nhis third, entitled “A Remark on the Relationship between\\nRelativity Theory and Idealistic Philosophy.” (Gödel\\n1949a). The latter paper coincided with results on rotating universes\\nin relativity he had obtained in 1949, which were first published in\\nan article entitled: “An Example of a New Type of Cosmological\\nSolutions of Einstein’s Field Equations of Gravitation.”\\n(Gödel 1949).  \\nAmong Gödel’s other significant philosophical works of the\\n1940s must be counted his 1941 lecture entitled “In What Sense\\nis Intuitionistic Logic Constructive?” (Gödel *1941) in\\nwhich the notion: “computable function of finite type” is\\nintroduced. A paper based on the ideas in the lecture entitled\\n“Über eine bisher noch nicht benützte Erweiterung des\\nfiniten Standpunktes,” was published only in 1958, and the\\ninterpretation of Heyting arithmetic into the quantifier free calculus in it became known as the “Dialectica\\nInterpretation,” after the journal in which the article was\\npublished (Gödel 1958). (For the revision of it from 1972, see\\nGödel 1995.) Finally the decade saw the beginning of\\nGödel’s intensive study of Leibniz, which, Gödel\\nreports, occupied the period from 1943 to\\n 1946.  \\nT  \\n[ ]  \\n4  \\nThe 1950s saw a deepening of Gödel’s involvement with\\nphilosophy: In 1951 Gödel delivered a philosophical lecture at\\nBrown University, usually referred to as the Gibbs Lecture, entitled\\n“Some Basic Theorems on the Foundations of Mathematics and Their\\nPhilosophical Implications” (Gödel *1951). From 1953 to\\n1959 Gödel worked on a submission to the Schilpp volume on Rudolf\\nCarnap entitled “Is Mathematics a Syntax of Language?”\\n(Gödel *1953/9-III, Gödel *1953/9-V). Gödel published\\nneither of these two important manuscripts in his lifetime, although\\nboth would appear on two lists which were found in the Gödel\\nNachlass, entitled “Was ich publizieren könnte.” (In\\nEnglish: “What I could publish.” Both manuscripts\\neventually appeared in Gödel 1995.) By the decade’s close\\nGödel developed a serious interest in\\n phenomenology.  \\n[ ]  \\n5  \\nGödel’s final years are notable for his circulation of two\\nmanuscripts: “Some considerations leading to the probable\\nconclusion that the true power of the continuum is\\nℵ ,” (Gödel *1970a, *1970b) his attempt\\nto derive the value of the continuum from the so-called scale axioms\\nof Hausdorff, and his “Ontologischer Beweis,” (Gödel\\n*1970) which he entrusted to Dana Scott in 1970 (though it appears to\\nhave been written earlier). Taken together, the two manuscripts are\\nthe fitting last words of someone who, in a fifty year involvement\\nwith mathematics and philosophy, pursued, or more precisely, for pursuing those two subjects under the\\nsingle heading: “strenge Wissenschaft”—a turn of\\nmind that had been in place from Gödel’s start in 1929,\\nwhen at the age of twenty-three he opened his doctoral thesis with\\nsome philosophical remarks.  \\n2  \\nsought the grounds  \\nGödel died in Princeton on January 14, 1978 at the age of 71. His\\ndeath certificate records the cause of death as “starvation and\\ninanition, due to personality disorder.” His wife Adele survived\\nhim by three years.  \\nFor further biographical material, see Gödel 1987, Kleene 1987,\\nKreisel 1980, Taussky-Todd 1987 and Yourgrau 2005.  \\n2. Gödel’s Mathematical Work  \\nBelow is an examination of some of Gödel’s main\\ncontributions in logic and set theory. This treatment of\\nGödel’s technical work is not exhaustive, omitting\\ndiscussion of Gödel’s work in physics and his work on the\\ndecision problem. These will be treated in the sequel to this\\nentry.  \\nFor a complete chronology of Gödel’s work the reader is\\nreferred to that compiled by John Dawson in volume I of\\nGödel’s Collected Works (Gödel 1986, p. 37).  \\n2.1 The Completeness Theorem  \\n2.1.1 Introduction  \\nThe completeness question for the first order predicate calculus was\\nstated precisely and in print for the first time in 1928 by Hilbert\\nand Ackermann in their text (Hilbert and Ackermann 1928), a text with which Gödel\\nwould have been quite\\n familiar.  \\nGrundzüge der theoretischen\\nLogik  \\n[ ]  \\n6  \\nThe question Hilbert and Ackermann pose is whether a certain\\nexplicitly given axiom system for the first order predicate calculus\\n“…is complete in the sense that from it all logical\\nformulas that are correct for each domain of individuals can be\\nderived…” (van Heijenoort 1967, p. 48).  \\n2.1.2 Proof of the Completeness Theorem  \\nWe give an outline of Gödel’s own proof in his doctoral\\nthesis (Gödel 1929). An essential difference with earlier efforts\\n(discussed below and elsewhere, e.g. in Zach 1999), is that Gödel\\ndefines meticulously all the relevant basic concepts.  \\nA “logical expression” in Gödel’s terminology\\nis a well-formed first order formula without identity. An expression\\nis “refutable” if its negation is provable,\\n“valid” if it is true in every interpretation and\\n“satisfiable” if it is true in some interpretation. The\\nCompleteness Theorem is stated as follows:  \\n. Every valid logical expression is provable. Equivalently, every\\nlogical expression is either satisfiable or refutable.  \\nTheorem 1  \\nGödel’s proof calculus is that of Hilbert and\\nAckermann’s text. An expression is in normal form if all the\\nquantifiers occur at the beginning. The degree of an expression or\\nformula is the number of alternating blocks of quantifiers at the\\nbeginning of the formula, assumed to begin with universal quantifiers.\\nGödel shows that if the completeness theorem holds for formulas\\nof degree it must hold for formulas of degree +\\n1. Thus the question of completeness reduces to formulas of degree 1.\\nThat is, it is to be shown that any normal formula ( )φ\\nof degree 1 is either satisfiable or refutable, where\\n“( )” stands for a (non-empty) block of universal\\nquantifiers followed by a (possibly empty) block of existential\\nones.  \\nk  \\nk  \\nQ  \\nQ  \\nGödel defines a book-keeping device, a well-ordering of all\\ntuples of variables arising from a need to satisfy φ as dictated\\nby ( ). For example, if ( )φ is\\n∀ ∃ ψ( , ), we list the quantifier-free formulas\\nψ( , ). (Or more precisely, finite\\nconjunctions of these in increasing length. See below.) Then in any\\ndomain consisting of the values of the different , in which each\\nψ( , ) is\\ntrue, the sentence ( )φ is clearly true. A crucial lemma\\nclaims the provability, for each , of the formula\\n( )φ →\\n( )φ , where the\\nquantifier free formula φ asserts the truth\\nof ψ for all tuples up to the kth tuple of variables arising from\\n( ), and\\n( )φ is the\\nexistential closure of φ . (See the example\\nbelow where the definition of the φ s\\nis given.) This lemma is the main step missing from the various\\nearlier attempts at the proof due to Löwenheim and Skolem, and,\\nin the context of the completeness theorem for first order logic,\\nrenders the connection between syntax and semantics completely\\nexplicit.  \\nQ  \\nQ  \\nx  \\n0  \\nx  \\n1  \\nx  \\n0  \\nx  \\n1  \\nx  \\nn  \\nx  \\n+1  \\nn  \\nx  \\nn  \\nx  \\nn  \\nx  \\nn+1  \\nQ  \\nk  \\nQ  \\nQ  \\nk  \\nk  \\nk  \\nQ  \\nQ  \\nk  \\nk  \\nk  \\n′  \\nk  \\nLet us consider an example of how a particular formula would be found\\nto be either satisfiable or its negation provable, following\\nGödel’s method: Consider φ =\\n∀ ∃ ψ( , ), where ψ( , ) is quantifier-free. We show that this is\\neither refutable or satisfiable. We make the following\\ndefinitions:  \\nx  \\n0  \\nx  \\n1  \\nx  \\n0  \\nx  \\n1  \\nx  \\n0  \\nx  \\n1  \\nφ is the expression\\nψ( , )  \\n0  \\nx  \\n0  \\nx  \\n1  \\nφ is the expression\\nψ( , ) ∧\\nψ( , )  \\n1  \\nx  \\n0  \\nx  \\n1  \\nx  \\n1  \\nx  \\n2  \\n…  \\nφ is the expression\\nψ( , ) ∧\\n…∧ ψ( , ).  \\nn  \\nx  \\n0  \\nx  \\n1  \\nx  \\nn  \\nx  \\n+1  \\nn  \\nThe crucial lemma, referred to above, shows that from φ we can\\nderive for each ,\\n∃ …∃ φ .  \\nn  \\nx  \\n0  \\nx  \\n+1  \\nn  \\nn  \\nFor some ,\\nφ is not satisfiable. Then, Gödel\\nargued, using the already known completeness theorem for propositional\\n logic, that ¬φ is provable, and hence so is\\n∀ ,…, ¬φ . Thus\\n¬∃ …∃ φ is provable and therefore the ¬φ is provable, i.e., φ is\\nrefutable in the Hilbert-Ackermann system. (Some partial results about\\npropositional logic in addition to those already mentioned include the\\nsemantic completeness of the propositional calculus due to Post\\n(1921), as well as a more general completeness theorem for the same\\ndue to Bernays in 1918; the latter appears in Bernays’\\nunpublished of 1918; see also Bernays\\n1926.)  \\nCase 1:  \\nn  \\nn  \\n[ ]  \\n7  \\nn  \\nx  \\n0  \\nx  \\n+1  \\nn  \\nn  \\nx  \\n0  \\nx  \\n+1  \\nn  \\nn  \\nHabilitationsschrift  \\nEach φ is\\nsatisfiable. There are only finitely many possible models with\\nuniverse { ,…, }.\\nGödel orders them as a tree by defining a model to be\\nbelow a model ′ if is a submodel of ′. In this way we obtain a tree which is finitely\\nbranching but infinite. By König’s Lemma there is an\\ninfinite branch . (In the proof, Gödel explicitly\\nconstructs the branch given by König’s Lemma rather than\\nciting it by name.) The union of the models on forms a\\nmodel with universe { , ,…}. Since satisfies each\\nφ , the original formula φ holds in . So φ is satisfiable and we are done.  \\nCase 2:  \\nn  \\nx  \\n0  \\nx  \\nn+1  \\nM  \\nM  \\nM  \\nM  \\nB  \\nB  \\nM  \\nx  \\n0  \\nx  \\n1  \\nM  \\nn  \\nM  \\nNote that the model, in the satisfiability case of Gödel’s\\nproof, is always countable. Thus this proof of the Completeness\\nTheorem gives also the Löweheim-Skolem Theorem (see below).\\nGödel extends the result to countably many formulas and to the\\ncase of first order logic with identity. He also proves the\\nindependence of the axioms.  \\nIn 1930 Gödel published the paper based on his thesis (Gödel\\n1930) notable also for the inclusion of the compactness theorem, which\\nis only implicitly stated in the thesis. The theorem as stated by\\nGödel in Gödel 1930 is as follows: a countably infinite set\\nof quantificational formulas is satisfiable if and only if every\\nfinite subset of those formulas is satisfiable. Gödel uses\\ncompactness to derive a generalization of the completeness\\ntheorem.  \\nThe Compactness Theorem was extended to the case of uncountable\\nvocabularies by Maltsev in 1936 (see Mal’cev 1971), from which\\nthe Upward Löwenheim-Skolem theorem immediately follows. The\\nCompactness Theorem would become one of the main tools in the then\\nfledgling subject of model theory.  \\n2.1.3 An Important Consequence of the Completeness Theorem  \\nA theory is said to be categorical if it has only one model up to\\nisomorphism; it is λ-categorical if it has only one model of\\ncardinality λ, up to isomorphism. One of the main consequences\\nof the completeness theorem is that categoricity fails for Peano\\narithmetic and for Zermelo-Fraenkel set theory.  \\nIn detail, regarding the first order Peano axioms (henceforth ), the existence of non-standard models of them actually\\nfollows from completeness together with compactness. One constructs\\nthese models, which contain infinitely large integers, as follows: add\\na new constant symbol to the language of arithmetic. Extend to a new theory * by adding to it the infinite\\ncollection of axioms: { > , > , …}, where, e.g., is S(S(S(0))). *\\nis finitely consistent (i.e., every finite subset of * is\\nconsistent) hence consistent, hence by the Completeness Theorem it has\\na model.  \\nPA  \\nc  \\nPA  \\nPA  \\nc  \\n0  \\nc  \\n1  \\n3  \\nPA  \\nPA  \\nThis simple fact about models of Peano arithmetic was not pointed out\\nby Gödel in any of the publications connected with the\\nCompleteness Theorem from that time, and it seems not to have been\\nnoticed by the general logic community until much later.\\nSkolem’s definable ultrapower construction from 1933 (see Skolem\\n1933) gives a direct construction of a non-standard model of True\\nArithmetic (which extends Peano arithmetic, being the set of\\narithmetic sentences true in the natural numbers). But Skolem never\\nmentions the fact that the existence of such models follows from the\\ncompleteness and compactness theorems. Gödel in his review\\n(1934c) of Skolem’s paper also does not mention this fact,\\nrather observing that the failure of categoricity for arithmetic\\nfollows from the theorem.  \\nincompleteness  \\nAs for set theory, the failure of categoricity was already taken note\\nof by Skolem in 1923, because it follows from the\\nLöwenheim-Skolem Theorem (which Skolem arrived at that year; see\\nSkolem 1923, based on Löwenheim 1915 and Skolem 1920): any first\\norder theory in a countable language that has a model has a countable\\nmodel.  \\nSkolem’s observation that categoricity fails for set theory\\nbecause it has countable models is now known as the Skolem\\n paradox. The\\n observation is strongly emphasized in Skolem’s paper, which is\\naccordingly entitled ‘An Observation on the Axiomatic\\nFoundations of Set Theory’ As he wrote in the conclusion of it,\\nhe had not pointed out the relativity in set theory already in 1915\\nbecause:  \\n[ ]  \\n8  \\n… first, I have in the meantime been occupied with other\\nproblems; second, I believed that it was so clear that axiomatization\\nin terms of sets was not a satisfactory ultimate foundation of\\nmathematics that mathematicians would, for the most part, not be very\\nmuch concerned with it. But in recent times I have seen to my surprise\\nthat so many mathematicians think that these axioms of set theory\\nprovide the ideal foundation for mathematics; therefore it seemed to\\nme that the time had come to publish a critique. (English translation\\ntaken from van Heijenoort 1967, p. 300.)  \\nAs an aside, in the proof of the Löwenheim-Skolem theorem,\\nspecifically that part of the theorem in which one constructs a model\\nfor a satisfiable sentence, Löwenheim and Skolem’s tree\\nconstruction was more or less the same as appears in\\nGödel’s thesis. In a 1967 letter to Hao Wang, Gödel\\ntakes note of the fact that his completeness proof had almost been\\nobtained by Skolem in 1923. Though van Heijenoort and Dreben (Dreben\\nand van Heijenoort 1986) remark that “Throughout much of the\\n1920s it was not semantic completeness but the decision problem for\\nquantificational validity, a problem originating from the work of\\nSchröder and Löwenheim, that was the dominant concern in\\nstudying quantification theory” (examples of such results would\\ninclude the decision procedure for the first order monadic predicate\\ncalculus due to Behmann, (Behmann 1922)), according to Gödel, the\\nreasons that Skolem did not obtain the complete proof are different\\nand philosophically important, having to do with the then dominant\\nbias against semantics and against infinitary methods:  \\nThe Completeness Theorem, mathematically, is indeed an almost trivial\\nconsequence of Skolem 1923. However, the fact is that, at that time,\\nnobody (including Skolem himself) drew this conclusion neither from\\nSkolem 1923 nor, as I did, from similar considerations of his own\\n…This blindness (or prejudice, or whatever you may call it) of\\nlogicians is indeed surprising. But I think the explanation is not\\nhard to find. It lies in the widespread lack, at that time, of the\\nrequired epistemological attitude toward metamathematics and toward\\nnon-finitary reasoning. (Gödel 2003b).  \\nThe matter of Skolem’s contribution to the Completeness Theorem\\nhas been extensively discussed in van Atten and Kennedy 2009, as well\\nas in van Atten 2005.  \\n2.2 The Incompleteness Theorems  \\nGödel mentioned the possibility of the unsolvability of a\\nquestion about the reals already in his 1929 thesis, in arguing\\nagainst the formalist principle of Hilbert’s, that consistency\\nis a criterion for existence. In fact, giving a finitary proof of the\\nconsistency of analysis was a key desideratum of what was then known\\nas the Hilbert program, along with proving its completeness.\\nAccordingly it was Gödel’s turn to these questions,\\nespecially the first, which led him to the two incompleteness\\ntheorems. (For a discussion of the Hilbert Program the reader is\\nreferred to the standard references: Sieg 1990, 1988, 1999; Mancosu\\n1998, Zach 2003, Tait 1981 and Tait 2002.)  \\nThe First Incompleteness Theorem provides a counterexample to\\ncompleteness by exhibiting an arithmetic statement which is neither\\nprovable nor refutable in Peano arithmetic, though true in the\\nstandard model. The Second Incompleteness Theorem shows that the\\nconsistency of arithmetic cannot be proved in arithmetic itself. Thus\\nGödel’s theorems demonstrated the infeasibility of the\\nHilbert program, if it is to be characterized by those particular\\ndesiderata, consistency and completeness.  \\nAs an aside, von Neumann understood the two theorems this way, even\\nbefore Gödel did. In fact von Neumann went much further in taking\\nthe view that they showed the infeasibility of classical mathematics\\naltogether. As he wrote to Carnap in June of 1931:  \\nThus today I am of the opinion that 1. Gödel has shown the\\nunrealizability of Hilbert’s program. 2. There is no more reason\\nto reject intuitionism (if one disregards the aesthetic issue, which\\nin practice will also for me be the decisive factor). Therefore I\\nconsider the state of the foundational discussion in Königsberg\\nto be outdated, for Gödel’s fundamental discoveries have\\nbrought the question to a completely different\\n level.  \\n[ ]  \\n9  \\nAnd the previous fall von Neumann had written to Gödel in even\\nstronger terms:  \\nThus, I think that your result has solved negatively the foundational\\nquestion: there is no rigorous justification for classical\\nmathematics. (Gödel 2003b, p. 339)  \\nIt would take Gödel himself a few years to see that those aspects\\nof the Hilbert Program had been decisively refuted by his results\\n(Mancosu 2004).  \\n2.2.1 The First Incompleteness Theorem  \\nIn his (Wang 1996) Hao Wang published the\\nfull text of material Gödel had written (at Wang’s request)\\nabout his discovery of the incompleteness theorems. This material had\\nformed the basis of Wang’s “Some Facts about Kurt\\nGödel,” and was read and approved by Gödel:  \\nLogical Journey  \\nIn the summer of 1930 I began to study the consistency problem of\\nclassical analysis. It is mysterious why Hilbert wanted to prove\\ndirectly the consistency of analysis by finitary methods. I saw two\\ndistinguishable problems: to prove the consistency of number theory by\\nfinitary number theory and to prove the consistency of analysis by\\nnumber theory … Since the domain of finitary number theory was\\nnot well-defined, I began by tackling the second half… I\\nrepresented real numbers by predicates in number theory… and\\nfound that I had to use the concept of truth (for number theory) to\\nverify the axioms of analysis. By an enumeration of symbols, sentences\\nand proofs within the given system, I quickly discovered that the\\nconcept of arithmetic truth cannot be defined in arithmetic. If it\\nwere possible to define truth in the system itself, we would have\\nsomething like the liar paradox, showing the system to be\\ninconsistent… Note that this argument can be formalized to show\\nthe existence of undecidable propositions without giving any\\nindividual instances. (If there were no undecidable propositions, all\\n(and only) true propositions would be provable within the system. But\\nthen we would have a contradiction.)… In contrast to truth,\\nprovability in a given formal system is an explicit combinatorial\\nproperty of certain sentences of the system, which is formally\\nspecifiable by suitable elementary means…  \\nWe see that Gödel first tried to reduce the consistency problem\\nfor analysis to that of arithmetic. This seemed to require a truth\\ndefinition for arithmetic, which in turn led to paradoxes, such as the\\nLiar paradox (“This sentence is false”) and Berry’s\\nparadox (“The least number not defined by an expression\\nconsisting of just fourteen English words”). Gödel then\\nnoticed that such paradoxes would not necessarily arise if truth were\\nreplaced by provability. But this means that arithmetic truth and\\narithmetic provability are not co-extensive — whence the First\\nIncompleteness Theorem.  \\nThis account of Gödel’s discovery was told to Hao Wang very\\nmuch after the fact; but in Gödel’s contemporary\\ncorrespondence with Bernays and Zermelo, essentially the same\\ndescription of his path to the theorems is given. (See Gödel\\n2003a and Gödel 2003b respectively.) From those accounts we see\\nthat the undefinability of truth in arithmetic, a result credited to\\nTarski, was likely obtained in some form by Gödel by 1931. But he\\nneither publicized nor published the result; the biases logicians had\\nexpressed at the time concerning the notion of truth, biases which\\ncame vehemently to the fore when Tarski announced his results on the\\nundefinability of truth in formal systems 1935, may have served as a\\ndeterrent to Gödel’s publication of that theorem.  \\n2.2.2 The proof of the First Incompleteness Theorem  \\nWe now describe the proof of the two theorems, formulating\\nGödel’s results in Peano arithmetic. Gödel himself\\nused a system related to that defined in Principia Mathematica, but\\ncontaining Peano arithmetic. In our presentation of the First and\\nSecond Incompleteness Theorems we refer to Peano arithmetic as , following Gödel’s notation.  \\nP  \\nBefore proceeding to the details of the formal proof, we define the\\nnotion of ω-consistency used by Gödel in the First\\nIncompleteness Theorem: is if ⊢ ¬φ( ) for all implies ⊬ ∃ φ( ).\\nNaturally this implies consistency and follows from the assumption\\nthat the natural numbers satisfy the axioms of Peano arithmetic.  \\nP  \\nω-consistent  \\nP  \\nn  \\nn  \\nP  \\nx  \\nx  \\nOne of the main technical tools used in the proof is , a mechanism which assigns natural numbers to terms and\\nformulas of our formal theory . There are different ways of\\ndoing this. The most common is based on the unique representation of\\nnatural numbers as products of powers of primes. Each symbol of number theory is assigned a positive natural number\\n#( ) in a fixed but arbitrary way, e.g.  \\nGödel\\nnumbering  \\nP  \\ns  \\ns  \\n#(0) = 1  \\n#(=) = 5  \\n#(¬) = 9  \\n#(1) = 2  \\n#(\\u2009(\\u2009) = 6  \\n#(∀) = 10  \\n#(+) = 3  \\n#(\\u2009)\\u2009) = 7  \\n#( ) = 11 +  \\nv  \\ni  \\ni  \\n#(×) = 4  \\n#(∧) = 8  \\nThe natural number corresponding to a sequence = < ,…, >\\nof symbols is  \\nw  \\nw  \\n0  \\nw  \\nk  \\n=\\n2 ·\\n3 · … · ,  \\n⌈  \\nw  \\n⌉  \\n#( )  \\nw  \\n0  \\n#( )  \\nw  \\n1  \\np  \\nk  \\n#( )  \\nw  \\nk  \\nwhere is the +1st prime. It\\nis called its Gödel number and denoted by . In this way we can\\nassign Gödel numbers to formulas, sequences of formulas (once a\\nmethod for distinguishing when one formula ends and another begins has\\nbeen adopted), and most notably, proofs.  \\np  \\nk  \\nk  \\n⌈  \\nw  \\n⌉  \\nAn essential point here is that when a formula is construed as a\\nnatural number, then the numeral corresponding to that natural number\\ncan occur as the argument of a formula, thus enabling the syntax to\\n“refer” to itself, so to speak (i.e., when a numeral is\\nsubstituted into a formula the Gödel number of which the numeral\\nrepresents). This will eventually allow Gödel to formalize the\\nLiar paradox (with “provability” in place of\\n“truth”) by substituting into the formula which says,\\n‘the formula, whose code is , is unprovable,’\\nits own natural number code (or more precisely the corresponding\\nnumeral).  \\nx  \\nAnother concept required to carry out the formalization is the concept\\nof numeralwise expressibility of number theoretic predicates. A\\nnumber-theoretic formula φ( , …, ) is in if for each tuple of natural numbers\\n( , …, ):  \\nn  \\n1  \\nn  \\nk  \\nnumeralwise expressible  \\nP  \\nn  \\n1  \\nn  \\nk  \\n⊨\\nφ( , …, )  \\nN  \\nn  \\n1  \\nn  \\nk  \\n⇒  \\n⊢\\nφ( , …, )  \\nP  \\nn  \\n1  \\nn  \\nk  \\n⊨\\n¬φ( , …, )  \\nN  \\nn  \\n1  \\nn  \\nk  \\n⇒  \\n⊢\\n¬φ( , …, )  \\nP  \\nn  \\n1  \\nn  \\nk  \\nwhere is the formal term which denotes the natural\\nnumber . (In , this is ( (… (0)…), where is the number of iterations of the successor function applied to the\\nconstant symbol 0.) One of the principal goals is to numeralwise\\nexpress the predicate  \\nn  \\nn  \\nP  \\nS  \\nS  \\nS  \\nn  \\nPrf( , ): ‘the sequence with Gödel\\nnumber is a proof of the sentence with Gödel number .’  \\nx  \\ny  \\nx  \\ny  \\nReaching this goal involves defining forty-five relations, each\\ndefined in terms of the preceding ones. These relations are all\\nprimitive\\n recursive. Relations needed are, among others, those which assert of a natural\\nnumber that it codes a sequence, or a formula, or an axiom, or that it\\nis the code, denoted by\\nSb( ),\\nof a formula obtained from a formula with code by\\nsubstituting for its free variable the th numeral for = 1,\\n…, . The forty-fifth primitive recursive relation\\ndefined is Prf( , ), and the forty-sixth is  \\n[ ]  \\n10  \\nr  \\n…  \\nu  \\n1  \\nu  \\nn  \\n( )… ( )  \\nZ  \\nx  \\n1  \\nZ  \\nx  \\nn  \\nr  \\nu  \\ni  \\nx  \\ni  \\ni  \\nn  \\nx  \\ny  \\nProv( ): ‘the sentence with Gödel number is provable in ’  \\ny  \\ny  \\nP  \\nwhich without being primitive recursive, is however obtained from\\nPrf( , ) by existentially quantifying .\\n(Prov( ) satisfies only the ‘positive’ part of\\nnumeralwise expressibility, and not the negative part; but the\\nnegative part is not needed.)  \\nx  \\ny  \\nx  \\ny  \\nIn Theorem V of his paper, Gödel proves that any number theoretic\\npredicate which is primitive recursive is numeralwise expressible in . Thus since Prf( , ) and substitution\\nare primitive recursive, these are decided by when closed\\nterms are substituted for the free variables and . This is the heart of the matter as we will see. Another\\nkey point about numeralwise expressibility is that although we\\ninformally interpret, for example,\\nProv(Sb( )),\\nby: ‘the formula with Gödel number is provable\\nif the Gödel number for the th numeral is\\nsubstituted in place of the th variable,’ neither the\\nformal statement within the theory nor anything we prove\\nabout it appeals to such meanings. On the contrary\\nProv(Sb( )),\\nis a meaningless string of logical and arithmetical symbols. As\\nGödel puts it in his introduction to his theorem V, ‘The\\nfact that can be formulated vaguely by saying that every recursive\\nrelation is definable in the system (if the usual meaning\\nis given to the formulas of this system) is expressed in precise\\nlanguage, reference to any interpretation of the\\nformulas of , by the following Theorem (V) (Gödel 1986,\\np. 171, italics Gödel’s).  \\nP  \\nx  \\ny  \\nP  \\nx  \\ny  \\nr  \\n…  \\nu  \\n1  \\nu  \\nn  \\n( )… ( )  \\nZ  \\nx  \\n1  \\nZ  \\nx  \\nn  \\nr  \\nx  \\ni  \\ni  \\nP  \\nr  \\n…  \\nu  \\n1  \\nu  \\nn  \\n( )… ( )  \\nZ  \\nx  \\n1  \\nZ  \\nx  \\nn  \\nP  \\nwithout  \\nP  \\nGödel in his incompleteness theorems uses a method given in what\\nis called nowadays Gödel’s Fixed Point Theorem. Although\\nGödel constructs a fixed point in the course of proving the\\nincompleteness theorem, he does not state the fixed point theorem\\nexplicitly. The fixed point theorem is as follows:  \\n(Gödel’s Fixed Point Theorem) If φ( ) is a formula of number theory, then\\nthere is a sentence ψ such that ⊢ ψ ↔\\nφ( ), where is the formal term\\ncorresponding to the natural number code of ψ .  \\nTheorem 2  \\nv  \\n0  \\nP  \\n⌈  \\nψ  \\n⌉  \\n⌈  \\nψ  \\n⌉  \\n⌈  \\n⌉  \\nLet σ( , , ) be a\\nformula that numeralwise expresses the number theoretic predicate\\n‘ is the Gödel number of the formula obtained by\\nreplacing the variable in the formula whose\\nGödel number is by the term ’.\\nLet θ( ) be the formula\\n∃ (φ( ) ∧\\nσ( , , )). Let = θ( ) and ψ = θ( ). Now directly by the construction ⊢ ψ ↔\\nφ( ψ ).  \\nProof:  \\nx  \\ny  \\nz  \\ny  \\nv  \\n0  \\nx  \\nz  \\nv  \\n0  \\nv  \\n1  \\nv  \\n1  \\nv  \\n0  \\nv  \\n1  \\nv  \\n0  \\nk  \\n⌈  \\nv  \\n0  \\n⌉  \\nk  \\nP  \\n⌈  \\n⌉  \\nA sentence is refutable from a theory if its negation is provable. The\\nFirst Incompleteness Theorem as Gödel stated it is as\\nfollows:  \\n(Gödel’s First Incompleteness\\nTheorem) If is ω-consistent, then there is a sentence which is\\nneither provable nor refutable from .  \\nTheorem 3  \\nP  \\nP  \\nBy judicious coding of syntax referred to above, write\\na formula\\n Prf( , ) of number theory, representable in , so that  \\nProof:  \\nx  \\ny  \\n[ ]  \\n11  \\nP  \\ncodes a proof of φ ⇒ ⊢\\nPrf( , ).  \\nn  \\nP  \\nn  \\n⌈  \\nφ  \\n⌉  \\nand  \\ndoes not code a proof of φ ⇒ ⊢ ¬Prf( , ).  \\nn  \\nP  \\nn  \\n⌈  \\nφ  \\n⌉  \\nLet Prov( ) denote the formula ∃ Prf( , ) .\\n By Theorem 2 there is a sentence φ with the property  \\ny  \\nx  \\nx  \\ny  \\n[ ]  \\n12  \\n⊢ (φ ↔\\n¬Prov( )).  \\nP  \\n⌈  \\nφ  \\n⌉  \\nThus φ says ‘I am not provable.’ We now observe, if ⊢ φ, then by (1) there is such that ⊢ Prf( , ), hence ⊢ Prov( ), hence,\\nby (3) ⊢ ¬φ, so is inconsistent.\\nThus  \\nP  \\nn  \\nP  \\nn  \\n⌈  \\nφ  \\n⌉  \\nP  \\n⌈  \\nφ  \\n⌉  \\nP  \\nP  \\n⊬ φ  \\nP  \\nFurthermore, by (4) and (2), we have ⊢\\n¬Prf( , ) for all natural\\nnumbers . By ω-consistency ⊬\\n∃ Prf( , ). Thus (3) gives ⊬ ¬φ. We have shown that if is\\nω-consistent, then φ is independent of .  \\nP  \\nn  \\n⌈  \\nφ  \\n⌉  \\nn  \\nP  \\nx  \\nx  \\n⌈  \\nφ  \\n⌉  \\nP  \\nP  \\nP  \\nOn concluding the proof of the first theorem, Gödel remarks,\\n“we can readily see that the proof just given is constructive;\\nthat is … proved in an intuitionistically unobjectionable\\nmanner…” (Gödel 1986, p. 177). This is because, as\\nhe points out, all the existential statements are based on his theorem\\nV (giving the numeralwise expressibility of primitive recursive\\nrelations), which is intuitionistically unobjectionable.  \\n2.2.3 The Second Incompleteness Theorem  \\nThe Second Incompleteness Theorem establishes the unprovability, in\\nnumber theory, of the consistency of number theory. First we have to\\nwrite down a number-theoretic formula that expresses the consistency\\nof the axioms. This is surprisingly simple. We just let\\nCon( ) be the sentence ¬Prov( ).  \\nP  \\n⌈  \\n0 =\\n1  \\n⌉  \\n(Gödel’s Second Incompleteness\\nTheorem) If is consistent, then Con( ) is not\\nprovable from .  \\nTheorem 4  \\nP  \\nP  \\nP  \\nLet φ be as in (3). The reasoning used to infer\\n‘if ⊢ φ, then ⊢ 0 ≠\\n1‘ does not go beyond elementary number theory, and can\\ntherefore, albeit with a lot of effort (see below), be formalized in . This yields: ⊢\\n(Prov( ) →\\n¬Con( )), and thus by (3), ⊢\\n(Con( ) → φ). Since ⊬ φ, we\\nmust have ⊬ Con( ).  \\nProof:  \\nP  \\nP  \\nP  \\nP  \\n⌈  \\nφ  \\n⌉  \\nP  \\nP  \\nP  \\nP  \\nP  \\nP  \\nThe above proof (sketch) of the Second Incompleteness Theorem is\\ndeceptively simple as it avoids the formalization. A rigorous proof\\nwould have to establish the proof of ‘if ⊢\\nφ, then ⊢ 0 ≠ 1’ in .  \\nP  \\nP  \\nP  \\nIt is noteworthy that ω-consistency is not needed in the proof\\nof Gödel’s Second Incompleteness Theorem. Also note that\\nneither is ¬Con( ) provable, by the consistency of and the fact, now known as Löb’s theorem, that ⊢\\nProv( ) implies ⊢ φ.  \\nP  \\nP  \\nP  \\n⌈  \\nφ  \\n⌉  \\nP  \\nThe assumption of ω-consistency in the First Incompleteness\\nTheorem was eliminated by Rosser in 1936, and replaced by the weaker\\nnotion of consistency. Rosser’s generalization involves applying\\nthe fixed point theorem to the formula ( ):\\n‘for all : either is not the Gödel\\nnumber of a proof of the formula with Gödel number or\\nthere is a proof shorter than of the negation of (the\\nformula with Gödel number) ’ (see Rosser\\n1936).  \\nR  \\nx  \\nz  \\nz  \\nx  \\nz  \\nx  \\nWith regard to the Second Incompleteness Theorem, the argument relies\\nin part on formalizing the proof of the First Incompleteness Theorem\\nas we saw. This step is omitted in Gödel 1931. He planned to\\ninclude the step in what would have been a second part II (see\\nfootnote 48a of Gödel 1931). But instead of writing it he turned\\nto the continuum\\n problem. (Part II was to elaborate on other points too: the ‘true reason\\nfor incompleteness,’ and the applicability of the two theorems\\nto other systems.) He perhaps did not feel compelled to attend to what\\nlooked like an exercise in formalization, relying instead on the\\ninformal argument to convince (in which it succeeded). However this\\nstep turned out to be somewhat non-trivial. As Kleene puts it in his\\nintroduction to Gödel 1931, of the informal presentation,\\n“Certainly the idea of the argument for Theorem XI (consistency)\\nwas very convincing; but it turned out that the execution of the\\ndetails required somewhat more work and care than had been\\nanticipated.” (See pp. 126–141 of Gödel 1986.)\\nEventually a complete proof of the Second Theorem was given by Hilbert\\nand Bernays in some seventy pages in their Hilbert and Bernays 1939. A\\nmuch more compact treatment of the theorem was given by Löb in\\nhis Löb 1956, and subsequently Feferman, in his 1960\\n“Arithmetization of Metamathematics in a General Setting”\\n(Feferman 1960/1961), gave a succinct and completely general treatment\\nof both the First and Second Theorems. But see the supplementary\\ndocument:  \\n[ ]  \\n13  \\nDid the Incompleteness Theorems Refute Hilbert’s Program?  \\nFor more detailed discussion, see the entry on .  \\nGödel’s incompleteness theorems  \\n2.3 Speed-up Theorems  \\nGödel’s 1936 ‘Speed-up’ theorem, published in\\nan abstract “On the length of proofs”, Gödel 1936\\nsays that while some sentences of arithmetic are true but unprovable,\\nthere are other sentences which are provable, but even the shortest\\nproof is longer than any bound given in advance as a recursive\\nfunction of the sentence. More exactly:  \\n. Given any recursive function there are provable sentences\\nφ of arithmetic such that the shortest proof is greater than ( φ ) in length.  \\nTheorem 5  \\nf  \\nf  \\n⌈  \\n⌉  \\nThe proof we will outline is sensitive to the particular concept we\\nuse for the length of a proof. Another possibility, and the one that\\nGödel has in mind, is the number of formulas in the proof. Buss\\n(see below) proves the theorem in either case, so both cases are\\nresolved.  \\nLet be total recursive function. By\\nGödel’s Fixed Point theorem there is a formula\\nφ( ) stating ‘φ( ) has no proof in PA\\nshorter than ( )’. This is tenable if the\\nlength is measured by number of symbols, because we only need to\\nsearch through finitely many proofs shorter than ( ). Note that φ( ) is true for all , for if φ( ) were false, then there would be a\\nshort proof of φ( ), and hence by soundness\\nφ( ) would be true, a contradiction: φ( )\\nwould both true and false. This can be formalized in PA and thus we\\nget the result that for each the sentence φ( )\\nis provable in PA. Since φ( ) is true for all ,\\nit cannot have a proof in PA which is shorter than ( ).  \\nProof:  \\nf  \\nn  \\nn  \\nf  \\nn  \\nf  \\nn  \\nn  \\nn  \\nn  \\nn  \\nn  \\nn  \\nn  \\nn  \\nn  \\nn  \\nf  \\nn  \\nThe Speed-up Theorem is the result of contemplating and elaborating\\nthe proof of the incompleteness theorem. It applies the fixed-point\\ntechnique to the concept of unprovability by a short proof, as opposed\\nto the original idea of applying the fixed-point theorem to mere\\nunprovability. The proof has very much the same flavor as the proof of\\nthe incompleteness theorem. Interestingly, it dates from the same year\\nas the construction, due to Rosser, that eliminates the use of\\nω-consistency in the first Incompleteness Theorem; like the\\nSpeed-up Theorem of Gödel, Rosser’s construction exploits\\nthe issue of short and long proofs. Gödel never submitted a proof\\nfor the Speed-up Theorem. Over the years several related proofs were\\npublished, but the first full proof of Gödel’s original\\nresult was given only in 1994 by Sam Buss in his ‘On\\nGödel’s theorems on lengths of proofs I: Number of lines\\nand speedups for arithmetic.’ (Buss 1994). Buss also gives a\\nsecond proof of the theorem which avoids self-reference, following a\\ntechnique due to Statman. Gödel measures the length of proofs by\\nthe number of formulas; but there are also other possibilities, such\\nas the number of symbols in the proof. The case of the Speed-up\\nTheorem where the length of proof is measured by the number of symbols\\nwas proved by Mostowski in 1952 (Mostowski 1982). For proofs of\\nsimilar results see Ehrenfeucht and Mycieleski 1971, and Parikh 1971.\\nThough both measures may be equally natural candidates for measuring\\nthe length of a proof, proving the theorem for length measured by the\\nnumber of symbols avoids a technical complication introduced by the\\nother measure: there are only finitely many proofs with a given number\\nof symbols, whereas there are infinitely many proofs with a given\\nnumber of formulas.  \\nGödel states the Speed-up Theorem differently from the above. Let be the system of logic of the -th\\norder, the variables of the first level being thought of as ranging\\nover natural numbers. In this setting, variables of the second level\\nrange over sets of natural numbers and so on. Gödel’s\\nformulation is:  \\nS  \\nn  \\nn  \\n. Let be a natural number > 0. If is a\\ncomputable function, then there are infinitely many formulas , provable in , such that if is the length of the shortest proof of in and is the length of the\\nshortest proof of in ,\\nthen > ( ).  \\nTheorem 6  \\nn  \\nf  \\nA  \\nS  \\nn  \\nk  \\nA  \\nS  \\nn  \\nl  \\nA  \\nS  \\n+1  \\nn  \\nk  \\nf  \\nl  \\nThe idea is the following: Let\\nφ( ) be a formula, like above, for which\\nφ( ) does not have a short proof in for any . Suppose we have a\\nhigher type system in which we can\\nprove ∀ φ( ). This proof is of constant\\nlength. Thus each φ( ) is derivable from this universal\\nstatement by one application of the logical rule\\n∀ φ( ) → φ( ). Thus\\nφ( ) has in that system for all a short\\nproof.  \\nProof sketch:  \\nx  \\nm  \\nS  \\nn  \\nm  \\nS  \\n+1  \\nn  \\nx  \\nx  \\nm  \\nx  \\nx  \\nt  \\nm  \\nm  \\nWhat kind of stronger system can we have in which\\n∀ φ( ) is provable? We may consider\\nsecond order logic in which we can define a predicate ( ) for the set of natural numbers and furthermore\\ncan prove of a new predicate symbol ( ) that it\\nsatisfies the inductive clauses of the truth definition of first order\\nformulas of arithmetic, relativized to . Then the stronger\\nsystem can prove that provable first order sentences of arithmetic\\nsatisfy the predicate . By the above argument, we can\\nprove in the stronger system that ∀ φ( )\\nsatisfies . Then by adding a few lines we can prove each\\nφ( ) satisfies . Because of the nature of\\nφ( ), this implies the stronger system has a (short)\\nproof of φ( ). An alternative system is Peano’s\\naxioms PA in an extended language where we have a new predicate symbol and axioms stating that the predicate codes\\nthe satisfaction relation for all sentences of the vocabulary not\\ncontaining .  \\nx  \\nx  \\nN  \\nx  \\nTr  \\nx  \\nN  \\nTr  \\nx  \\nx  \\nTr  \\nn  \\nTr  \\nn  \\nn  \\nTr  \\nTr  \\nTr  \\n2.4 Gödel’s Work in Set theory  \\n2.4.1 The consistency of the Continuum Hypothesis and the Axiom of Choice  \\nGödel’s proof of the consistency of the continuum\\nhypothesis with the axioms of Zermelo-Fraenkel set theory is a tour de\\nforce and arguably the greatest achievement of his mathematical life.\\nThis is because aside from the arithmetization, virtually all of the\\ntechnical machinery used in the proof had to be invented ab\\ninitio.  \\nThe Continuum Hypothesis (henceforth ) was formulated by\\nGeorg Cantor, and was the first problem on Hilbert’s list of\\ntwenty-three unsolved problems as given in his famous address to the\\nInternational Mathematical Congress in Paris in 1900. The problem as\\nstated by Hilbert is as follows: Let be an infinite set of\\nreal numbers. Then is either countable, or has cardinality\\n2 , i.e., is in one-to-one\\ncorrespondence either with the set of natural numbers or with the set\\nof all real numbers (otherwise known as the continuum). Another way to\\nstate the continuum hypothesis is that (the first uncountably infinite\\ncardinal) ℵ =\\n2 .  \\nCH  \\nA  \\nA  \\nℵ  \\n0  \\nA  \\n1  \\nℵ  \\n0  \\nAs early as 1922 Skolem speculated that the was\\nindependent of the axioms for set theory given by Zermelo in 1908.\\nNevertheless Hilbert published a (false) proof of the in\\nHilbert 1926. In 1937 Gödel proved its consistency with the\\naxioms of set theory. (Henceforth we use the standard\\nabbreviations for Zermelo-Fraenkel set theory, , and\\nZermelo-Fraenkel set theory with the Axiom of Choice, .)\\nThe consistency of the negation of the was shown by Paul\\nCohen in 1961 (see Cohen 1963) and hence together with\\nGödel’s result one infers that the is\\nindependent of (and ).  \\nCH  \\nCH  \\nZF  \\nZF  \\nZFC  \\nCH  \\nCH  \\nZF  \\nZFC  \\nCohen invented an important new technique called forcing in the course\\nof proving his result; this technique is at present the main method\\nused to construct models of set theory. Forcing led to a revival of\\nformalism among set theorists, the plurality of models being an\\nindication of the “essential variability in set theory,”\\n(Dehornoy 2004) and away from the notion that there is an intended\\nmodel of set theory—a perspective Gödel advocated since at\\nleast 1947, if not\\n earlier. Recently there have been signs that the may again be\\ncoming to be regarded as a problem to be solved mathematically (with\\nthe help of course of some new evident axioms extending ZF). (See for\\nexample Woodin 2001a, 2002, 2001b, and Foreman 1998.) If any of the\\nproposed solutions gain acceptance, this would confirm\\nGödel’s view that the would eventually be\\ndecided by finding an evident extension of the ZF axioms for set\\ntheory. The program associated with this view is called\\n“Gödel’s Large Cardinal Program.”  \\n[ ]  \\n14  \\nCH  \\nCH  \\n2.4.2 Gödel’s Proof of the Consistency of the Continuum Hypothesis and the Axiom of Choice with the Axioms of Zermelo-Fraenkel Set Theory  \\nThe continuum problem is shown to be consistent with ZF by finding an\\nenumeration of the reals which is indexed by the countable ordinals, a\\nstrategy which had been recognized as a promising one already by\\n Hilbert. The problem, and the intuition behind the proof, is to build a\\n“small” model, one in which the absolute minimum number of\\nreals is allowed, while at the same time the model is large enough to\\nbe closed under all the operations the axioms assert to\\nexist.  \\n[ ]  \\n15  \\nZF  \\nGödel’s is a relative consistency proof, obtained by\\nconstructing a so-called “inner model” for together with the . An inner model is a subcollection of the collection of all sets (see below) which\\nsatisfies the axioms of when only sets in are\\nconsidered. Gödel’s inner model is called the (see below) and is denoted by . Whatever is true in an inner model is consistent with for the same reason that any theory with a model is\\nconsistent. An artifact of the construction is that the Axiom of\\nChoice (henceforth ) is satisfied in Gödel’s\\ninner model and hence the consistency of the with was established by Gödel. Later on it was shown by\\nSierpinski that the is actually a consequence of the\\nGeneralized Continuum Hypothesis or the which states\\nthat for each κ, 2 = κ (see\\nSierpinski 1947).  \\nZF  \\nCH  \\nM  \\nV  \\nZF  \\nM  \\ninner\\nmodel of constructible sets  \\nL  \\nZF  \\nAC  \\nAC  \\nZF  \\nAC  \\nGCH,  \\nκ  \\n+  \\nGödel published two versions of these theorems, in 1939 and in\\n1940, entitled “Consistency Proof for the Generalized Continuum\\nHypothesis,” and “The Consistency of the Axiom of Choice\\nand of the Generalized Continuum Hypothesis with the Axioms of Set\\nTheory,” respectively. Though completely definitive, the 1939\\nversion is lacking in a great many details, most notably the arguments\\nshowing that if is built inside itself, the same results; that is to say, the so-called absoluteness\\narguments are missing. Also missing are the details of the proofs that\\nthe axioms hold in . Unlike the case of the\\nSecond Incompleteness Theorem, however, Gödel subsequently gave a\\ncompletely detailed proof of the two theorems in the 1940 monograph.\\n(The 1940 proof differs substantially from the first version. For\\ndetails about the two proofs and the difference between them the\\nreader is referred to Solovay 1990 and Kanamori 2006.)  \\nL  \\nL  \\nL  \\nZF  \\nL  \\nWe now sketch the proof of the consistency of and of with , using modern terminology. Some\\npreliminary concepts before sketching the proof: We first define the\\nstratified set theoretic universe, denoted . ( is\\nalso known as the cumulative hierarchy.) It is obtained by iteration\\nof the power set operation (℘) beginning with the null set:  \\nCH  \\nAC  \\nZFC  \\nV  \\nV  \\nV  \\n0  \\n=  \\n∅,  \\nV  \\nα+1  \\n=  \\n℘( ),  \\nV  \\nα  \\nV  \\nγ  \\n=  \\n,  \\n∪  \\nβ<γ  \\nV  \\nβ  \\nwhere α, β are any ordinals, γ is a limit ordinal and\\n℘( ) denotes the power set of . Finally  \\nx  \\nx  \\nV  \\n=  \\n,  \\n∪  \\nα∈  \\nOrd  \\nV  \\nα  \\nwhere denotes the class of all ordinals.  \\nOrd  \\nThe constructible hierarchy is likewise defined by\\nrecursion on ordinals. But whereas the full power set operation is\\niterated to obtain the cumulative hierarchy, the levels of the\\nconstructible hierarchy are defined strictly predicatively, that is by\\nincluding at the next level only those sets which are first order\\ndefinable using parameters from the previous level. More exactly, let ( ) denote the set of all subsets of definable in the structure < , ∈ > by first order\\nformulas with parameters in . (For more on definability see\\nthe entry on in this encyclopedia.)  \\nL  \\nDef  \\nA  \\nA  \\nA  \\nA  \\nmodel theory  \\nWith this notation the constructible hierarchy is defined by induction\\nover the ordinals as follows:  \\nL  \\n0  \\n=  \\n∅,  \\nL  \\nα+1  \\n=  \\n( ),  \\nDef  \\nL  \\nα  \\nL  \\nγ  \\n=  \\n,  \\n∪  \\nα<γ  \\nL  \\nα  \\nL  \\n=  \\n,  \\n∪  \\nα∈  \\nOrd  \\nL  \\nα  \\nA set is said to be if ∈ . The axiom which states that all sets are\\nconstructible is denoted = and is called the\\nAxiom of Constructibility. Note that is a proper class and\\nnot a set; although as we will see, each is a set, and the predicate “ is constructible”\\nis actually a definable term of the language.  \\nx  \\nconstructible  \\nx  \\nL  \\nV  \\nL  \\nL  \\nL  \\nα  \\nx  \\nOur next task is to show that is a model of . A\\nset or a class is if elements of it are also\\nsubsets. By a meticulous transfinite induction, can be shown to be transitive for each α; and therefore\\nso is itself. This fact, together with the observation that\\nsome elementary closure properties hold in is enough to show that is a model of . (Indeed,\\nas it turns out, is the minimal transitive model of the axioms containing all the ordinals, and is therefore in\\nthis sense canonical.)  \\nL  \\nZF  \\ntransitive  \\nL  \\nα  \\nL  \\nL  \\n[ ]  \\n16  \\nL  \\nZF  \\nL  \\nZF  \\nIn detail, proving that the axioms, apart from the\\ncomprehension axiom, are true in , amounts to showing that,\\nroughly speaking, any set with a property that a axiom asserts to exist, can be seen to exist in by considering the relativization of the\\nproperty to . (A property is\\nrelativized to an inner model by replacing every quantifier\\n∃ φ by ∃ ( ∈ ∧ φ) and every quantifier ∀ φ\\nby ∀ ( ∈ → φ).) As\\nfor the comprehension axiom, verifying it requires showing that the\\nset asserted to exist is constructed at a particular successor level . Proving this requires an important\\nprinciple of set theory which in modern terminology is called the Levy\\n(or ) Reflection Principle. This principle says that any\\nstatement in the language of which is true in is already true on some level of any continuously increasing hierarchy\\nsuch as . (For the history of this principle, see Kanamori\\n2006.) The Levy Reflection Principle gives the level α at which\\nthe elements of the set are all constructed. Gödel did not\\nactually have the Levy Reflection Principle but used the argument\\nbehind the proof of the principle.  \\nZF  \\nL  \\nP  \\nZF  \\nL  \\nP  \\nL  \\nP  \\nL  \\nP  \\nM  \\nx  \\nx  \\nx  \\nM  \\nx  \\nx  \\nx  \\nM  \\nL  \\nα + 1  \\nZF  \\nZF  \\nV  \\nL  \\nOnce it is established that is a model of , one\\ncan now prove that both the and the hold in . To this end, one first shows that the definition of is for , where absoluteness is\\ndefined as follows: given a class , a predicate ( ) is said to be absolute for if and\\nonly if for all ∈ , ( )\\n↔ ( ).  \\nL  \\nZF  \\nCH  \\nAC  \\nL  \\nL  \\nabsolute  \\nL  \\nM  \\nP  \\nx  \\nM  \\nx  \\nM  \\nP  \\nx  \\nP  \\nM  \\nx  \\nProving that the predicate “ is constructible”\\nis absolute requires formalizing the notion of definability, which in\\nturn requires formalizing the notion of satisfaction. This is because\\nthe predicate “ is constructible” says of a set,\\nthat for some ordinal α, and for some formula φ with\\nparameters in , = { ∈ | ⊨ φ( )}. This part of the proof is tedious but\\nunproblematic.  \\nx  \\nx  \\nL  \\nα  \\nx  \\ny  \\nL  \\nα  \\nL  \\nα  \\ny  \\nOnce the absoluteness of is established, it follows that satisfies the axiom of constructibility if it is\\nrelativized to ; that is, ⊢\\n(V=L) . In particular, the axiom = is\\nconsistent if is.  \\nL  \\nZF  \\nL  \\nZF  \\nL  \\nV  \\nL  \\nZF  \\nWe now give the idea of the proof of and in + = . (For a detailed exposition of\\nthe proof, the reader is referred to the standard sources. See for\\nexample Devlin’s chapter on constructibility in Barwise 1977;\\nsee also Kunen 1983, and Jech 2003.)  \\nCH  \\nAC  \\nZF  \\nV  \\nL  \\nAs concerns the , the idea behind the proof of it in is simply the following: Gödel showed that assuming = , every real number occurs on some countable\\nlevel of the -hierarchy. Since every countable level is\\nitself countable (after all, there are only countably many possible\\ndefining formulas), and there are ω countable\\nlevels, there must be only ω real numbers.  \\nCH  \\nL  \\nV  \\nL  \\nL  \\n1  \\n1  \\nThe difficulty here, if not of the whole proof altogether, lies in\\nshowing that every real is constructed already on a countable level of\\nthe -hierarchy. To show this Gödel argued as follows:\\nSuppose is a real number thought of as a set of natural\\nnumbers. By a combination of the Levy Reflection principle and the\\nLöwenheim-Skolem Theorem there is a countable submodel < , ∈ > of < , ∈ > satisfying a\\nsufficiently large part of the axioms + = , such that belongs to .\\nBy a simple procedure < , ∈ > can be converted\\ninto a transitive model < , ∈ >. This procedure,\\nused by Gödel already in 1937, was explicitly isolated by\\nMostowski (Mostowski 1949). The resulting model is referred to as the\\nMostowski Collapse.  \\nL  \\nA  \\nM  \\nL  \\nfinite  \\nZF  \\nV  \\nL  \\nA  \\nM  \\nM  \\nN  \\nLet us pause to discuss this important technique. Suppose < , > is a well-founded model of the axiom of\\nextensionality. It is a consequence of the well-foundedness of the\\nbinary predicate on , and of the principle of\\ntransfinite recursion, that the equation π( ) =\\n{π( )\\u2009| ∈ ∧ } defines a unique function on . The range of π is transitive, for if π( ) ∈ and ∈ π( ), then =\\nπ( ) for some ∈ with , whence π( ) ∈ . The fact that\\nπ is an isomorphism between < , > and\\n< , ∈ > can be proved by transfinite induction on\\nelements on , based again on the well-foundedness of . The well-foundedness of < , > is\\nin practice often the consequence of < , >\\nbeing a submodel of some < , ε\\n>.  \\nM  \\nE  \\nE  \\nM  \\nx  \\ny  \\ny  \\nM  \\nyEx  \\nM  \\nN  \\na  \\nN  \\ny  \\na  \\ny  \\nb  \\nb  \\nM  \\nbEa  \\nb  \\nN  \\nM  \\nE  \\nN  \\nM  \\nE  \\nM  \\nE  \\nM  \\nE  \\nV  \\nα  \\nWe now return to the proof of the in . We used\\nthe Mostowski Collapse to construct the transitive set . As\\nit turns out, the real number is still an element of < , ∈ > . By basic properties of , < , ∈ > must be < ,\\n∈ > for some α . Since is countable, α\\nis countable too. (It can be shown that | |\\n= |α| + ℵ .) Thus is constructible\\non a countable level, which was to have been shown.  \\nCH  \\nL  \\nN  \\nA  \\nN  \\nL  \\nN  \\nL  \\nα  \\nN  \\nL  \\nα  \\n0  \\nA  \\nAs for the , Gödel exhibits a definable well-ordering,\\nthat is, a formula of set theory which defines, in , a\\nwell-ordering of all of . The formula is tedious to write\\ndown but the idea is a simple one: A set precedes a set in the well-ordering if and only if either occurs in the -hierarchy on an earlier level than , or else they occur on\\nthe same level but is defined by a shorter formula than , or else they are defined by the same formula but the\\nparameters in the definition of occur in earlier\\nthan the parameters of . This well-ordering of shows that the holds in .  \\nAC  \\nL  \\nL  \\nx  \\ny  \\nx  \\nL  \\nL  \\nα  \\ny  \\nx  \\ny  \\nx  \\nL  \\ny  \\nL  \\nAC  \\nL  \\nThis concludes the proof of the consistency of and the in .  \\nAC  \\nCH  \\nL  \\nWe note that Gödel proved more in his 1939 and 1940 than what was\\nshown here, namely he proved the Generalized Continuum Hypothesis in and hence that its consistency with .  \\nL  \\nZF  \\n2.4.3 Consequences of Consistency  \\nAs noted above, it was suggested already in the 1920s that the might be independent of or . After\\nfirst conjecturing that the Axiom of Constructibility might be\\n“absolutely consistent,” meaning not falsifiable by any\\nfurther extension of models of + = , in his 1947 “What is Cantor’s Continuum\\nHypothesis?” Gödel conjectured that the would\\nbe shown to be independent. The main consequence of Gödel’s\\nresult, then, as far as the problem of proving the independence of the is concerned, was that it pointed mathematicians in the\\ndirection of adding non-constructible sets to a model of set theory in\\norder to establish the consistency of the negation of the .\\nIn 1961 Dana Scott proved that the failure of the Axiom of\\nConstructibility follows from the existence of a measurable cardinal,\\ncontrary to a conjecture Gödel had made in 1940. (See Scott 1961.\\nA cardinal κ is said to be measurable if there is a\\nnon-principal κ-complete ultrafilter in the power-set Boolean\\nalgebra of κ.) In 1963, as noted, Paul Cohen proved the\\nconsistency of the negation of the by adding\\nnon-constructible sets to an inner model.  \\nCH  \\nZF  \\nZFC  \\nZF  \\nV  \\nL  \\n[ ]  \\n17  \\nCH  \\nCH  \\nCH  \\nCH  \\nWhat other open questions of set theory could be solved by\\nGödel’s method? Gödel himself noted some consequences.\\nThey are related to so called projective sets of real numbers and\\nfinite sequences of real numbers. The simplest projective sets are the\\nclosed sets, also called Π -sets. A set is\\nΣ if it is the projection of\\na Π -subset of the real plane. A\\nset is Δ if it and its\\ncomplement are Σ . Gödel\\nobserved that there is both a non-Lebesgue measurable\\nΔ -set and an uncountable\\nΠ -set without a perfect subset in . (A set of reals is perfect if it is closed, non-empty, and\\nhas no isolated points. Such sets have the size of the continuum.)\\nGödel gave a sketch of the proof in the 1951 second printing of\\nGödel 1940.  \\n1  \\n0  \\n1  \\n+1  \\nn  \\n1  \\nn  \\n1  \\n+1  \\nn  \\n1  \\n+1  \\nn  \\n1  \\n2  \\n1  \\n1  \\nL  \\nIt has turned out subsequently that the axiom = gives a virtually complete extension of . This means that,\\napart from sentences arising from Gödel’s incompleteness\\ntheorems, essentially all set-theoretical questions can be decided by\\nmeans of the axioms = . This is not to imply that\\nsuch results are in any way trivial. Indeed, it has turned out that is quite a complicated structure, despite its relatively\\nsimple description. As for settling open set-theoretical questions in the main step was the emergence of Jensen’s fine\\nstructure theory of (Jensen 1972). Recalling that the\\nsuccessor step in the definition of\\nthe constructible hierarchy adds to all subsets of definable by first order formulas φ\\nover ( , ∈), fine structure theory,\\nroughly speaking, ramifies the step from to into smaller steps according to the\\ncomplexity of the defining formula φ. Jensen established by means\\nof his fine structure a strengthening, denoted by ◊, of , that he used to construct a Souslin tree in ,\\nand a combinatorial principle □ that he used to show that the\\nSouslin Hypothesis is consistent with .  \\nV  \\nL  \\nZFC  \\nV  \\nL  \\nL  \\nL,  \\nL  \\nL  \\nα +1  \\nL  \\nL  \\nα  \\nL  \\nα  \\nL  \\nα  \\nL  \\nα+1  \\nCH  \\nL  \\nCH  \\n2.4.4 Gödel’s view of the Axiom of Constructibility  \\nIf he did not think this way from the outset, Gödel soon came to\\nadopt the view that the Axiom of Constructibility was implausible. As\\nhe remarked at the end of his 1947 “What is Cantor’s\\nContinuum Hypothesis?”  \\n…it is very suspicious that, as against the numerous plausible\\npropositions which imply the negation of the continuum hypothesis, not\\none plausible proposition is known which would imply the continuum\\nhypothesis. (Gödel 1990, p. 186)  \\nGödel was compelled to this view of by the\\n Leibnizian idea that, rather than the universe being “small,” that\\nis, one with the minimum number of sets, it is more natural to think\\nof the set theoretic universe as being as large as\\n possible. This\\n idea would be reflected in his interest in maximality principles,\\ni.e., principles which are meant to capture the intuitive idea that\\nthe universe of set theory is maximal in the sense that nothing can be\\nadded; and in his conviction that maximality principles would\\neventually settle statements like the . As Gödel put\\nit in a letter to Ulam in the late 1950s, about a maximality principle\\nof von Neumann:  \\nL  \\n[ ]  \\n18  \\n[ ]  \\n19  \\nCH  \\nThe great interest which this axiom has lies in the fact that it is a\\nmaximality principle, somewhat similar to Hilbert’s axiom of\\ncompleteness in geometry. For, roughly speaking, it says that any set\\nwhich does not, in a certain well defined way, imply an inconsistency\\nexists. Its being a maximum principle also explains the fact that this\\naxiom implies the axiom of choice. I believe that the basic problems\\nof set theory, such as Cantor’s continuum problem, will be\\nsolved satisfactorily only with the help of stronger axioms of this\\nkind, which in a sense are opposite or complimentary to the\\nconstructivistic interpretation of mathematics. (Ulam 1958, as quoted\\nin Gödel 1990, p. 168; original emphasis. Note that this is\\ndifferent from the very similar passage Gödel 2003b, p.295.)  \\nTwenty years earlier, in 1938, Gödel had written seemingly\\ndifferently about the Axiom of Constructibility:  \\nThe proposition (i.e., = ) added as a\\nnew axiom seems to give a natural completion of the axioms of set\\ntheory, in so far as it determines the vague notion of an arbitrary\\ninfinite set in a definite way. (Gödel 1986, p.27)  \\nA  \\nV  \\nL  \\nGödel may have meant by “natural completion” here\\n“the correct completion,” or he may have meant to say no\\nmore than that the Axiom of Constructibility determines the notion of\\nset in a definite way. In any case he used the term\\n“natural” differently in a conversation with Wang on\\nconstructibility in 1972 (Wang 1996, p. 144):  \\nGödel talked more about the relation between axioms of infinity\\nand the constructible universe…(he observed that) preliminary\\nconcepts such as that of constructible sets are necessary to arrive at\\nthe natural concept, such as that of set.  \\nThis is reminiscent of a remark of Hugh Woodin, that studying forcing\\nleads to a better understanding of — the general\\nprinciple being that studying the models of a theory is not only\\nuseful to understand the theory itself, but useful to obtain a better\\npicture of (Woodin 1988).  \\nV  \\nV  \\nFor more on Gödel’s program and on Gödel’s\\nprogram relative to the the reader is referred e.g., to\\nSteel forthcoming and Feferman . 2000. For more on\\nGödel’s result, its history , and its significance the\\nreader is referred to Floyd/Kanamori 2006 and Kennedy 2006.  \\nCH  \\net al  \\n2.5 Gödel’s Work in Intuitionistic Logic and Arithmetic  \\nGödel’s interest in intuitionism was deep and long-lasting.\\nAlthough he himself did not subscribe to that view, he made a number\\nof important contributions to intuitionistic logic. Perhaps the\\nimportance he placed on the concept of evidence (see below) led to his\\nclose consideration of it.  \\nWe discuss Gödel’s results on intuitionistic logic in their\\nchronological order.  \\n2.5.1 Intuitionistic Propositional Logic is not Finitely-Valued  \\nBoth many-valued logic, introduced by Łukasiewicz in the twenties\\n(Łukasiewicz 1970) and intuitionistic logic, formalized by\\nHeyting in 1930, fail to satisfy the law of excluded middle. It was\\ntherefore natural to ask whether intuitionistic logic can be presented\\nas a many-valued logic, and indeed a number of logicians in the 1920s\\nhad suggested just that. In his 1932 Gödel gave a simple argument\\nwhich shows that intuitionistic propositional logic cannot be thought\\nof as a finitely-valued logic. Precisely, Gödel proved two\\ntheorems:  \\n. There is no realization with finitely many elements (truth values) for\\nwhich the formulas provable in , and only those, are\\nsatisfied (that is, yield designated values for an arbitrary\\nassignment).  \\nTheorem 7  \\nH  \\n( is intuitionistic propositional logic, after\\nHeyting.)  \\nH  \\n. Infinitely many systems lie between and the system of the ordinary propositional calculus, that is,\\nthere is a monotonically decreasing sequence of systems all of which\\ninclude as a subset and are included in as subsets.  \\nTheorem 8  \\nH  \\nA  \\nH  \\nA  \\nIn his proof he considered for each natural number > 0\\nthe sentence  \\nn  \\n= ≡ .  \\nF  \\nn  \\n∨  \\n1\\n≤ < ≤  \\ni  \\nj  \\nn  \\np  \\ni  \\np  \\nj  \\nHe observed that in an -valued logic the sentences , for > ,\\nshould be derivable. However, Gödel showed, is not derivable from Heyting’s\\naxioms for any .  \\nn  \\nF  \\nm  \\nm  \\nn  \\nF  \\nn  \\nn  \\nSubsequently Jaśkowski (Jaśkowski 1936) showed that\\nintuitionistic propositional logic can be given a many-valued\\nsemantics in terms of infinitely many truth-values. For further\\ndiscussion of many-valued logics, see for example the entry on in this encyclopedia as well as van Stigt’s article on\\nintuitionistic logic in Mancosu 1998.  \\nmany-valued logic  \\n2.5.2 Classical Arithmetic is Interpretable in Heyting Arithmetic  \\nWe now consider Gödel 1933e, in which Gödel showed, in\\neffect, that intuitionistic or Heyting arithmetic is only apparently\\nweaker than classical first-order arithmetic. This is because the\\nlatter can be interpreted within the former by means of a simple\\ntranslation, and thus to be convinced of the consistency of classical\\narithmetic, it is enough to be convinced of the consistency of Heyting\\narithmetic. Heyting arithmetic is defined to be the same as classical\\narithmetic, except that the underlying predicate logic is given by\\nintuitionistic axioms and rules of inference (see below).  \\nThis result extends the same assertion for the propositional case. Let denote the intuitionistic propositional logic, and denote its classical counterpart (as above).\\nInductively define:  \\nH  \\nA  \\n′  \\nA  \\n≡  \\n¬¬ ( atomic)  \\nA  \\nA  \\n(¬ )′  \\nA  \\n≡  \\n¬ ′  \\nA  \\n( → )′  \\nA  \\nB  \\n≡  \\n¬( ′ ∧\\n¬ ′)  \\nA  \\nB  \\n( ∨ )′  \\nA  \\nB  \\n≡  \\n¬(¬ ′ ∧\\n¬ ′)  \\nA  \\nB  \\n( ∧ )′  \\nA  \\nB  \\n≡  \\n′ ∧ ′  \\nA  \\nB  \\nThen,  \\n. Let be a propositional formula. Then ⊢ if and only if ⊢ ′,  \\nTheorem 9  \\nF  \\nH  \\nF  \\nA  \\nF  \\nThe theorem follows easily from the result of Glivenko (1929) that\\n¬ follows from if and only if\\n¬ follows from , for any propositional\\nformula .  \\nF  \\nH  \\nF  \\nA  \\nF  \\nGödel’s so-called double negation interpretation extends\\nTheorem 9 to a reduction of classical first order logic to\\nintuitionistic predicate logic. The translation in this case can be\\ntaken to map ′ to for atomic .\\nMoreover, we let ∀ ( )′ =\\n∀ ′( ) :  \\nA  \\nA  \\nA  \\nxA  \\nx  \\nxA  \\nx  \\n. Suppose is a first order formula. If is provable\\nin classical first order logic, then ′ is provable in\\nintuitionistic first order logic.  \\nTheorem 10  \\nA  \\nA  \\nA  \\nThe above result had been obtained independently by Gentzen (with\\nBernays), but upon hearing of Gödel’s result Gentzen\\nwithdrew his paper from publication. It had also been anticipated by\\nKolmogorov in his 1925 “On the Principle of the Excluded\\nMiddle,” (English translation van Heijenoort 1967) but that\\npaper was largely unknown to logicians who were outside of\\nKolmogorov’s circle.  \\nBernays has written (see Bernays’ entry on David Hilbert in\\nEdwards 1967) that this result of Gödel’s drew the\\nattention of the Hilbert school to two observations: first, that\\nintuitionistic logic goes beyond finitism, and secondly, that finitist\\nsystems may not be the only acceptable ones from the foundational\\npoint of view.  \\nThe following theorem for the case of arithmetic follows from Theorem\\n10:  \\n. Suppose is a first order formula of arithmetic. If is provable in classical Peano arithmetic, then ′ is provable in intuitionistic first order\\narithmetic.  \\nTheorem 11  \\nA  \\nA  \\nA  \\nFor a list of the axioms and rules of intuitionistic first order logic\\nsee Gödel 1958, reprinted with detailed introductory note by A.S.\\nTroelstra in Gödel 1990. See also Troelstra 1973, and\\nTroelstra’s “Aspects of constructive mathematics” in\\nBarwise 1977. For a detailed proof of the above theorem the reader is\\nreferred also to the latter.  \\n2.5.3 Intuitionistic Propositional Logic is Interpretable in  \\nS4  \\nThis result of Gödel’s (Gödel 1933f), which marks the\\nbeginning of provability logic, makes exact the difference between the\\nconcept of “provability in a specified formal system” and\\nthat of “provability by any correct means.”  \\nGödel had already noted this difference in the introduction to\\nhis 1929 thesis. The context was the following: Gödel entertains\\nthere the possibility that his proof of the Completeness Theorem might\\nbe circular, since the law of excluded middle was used to prove it.\\nThis is because while the Completeness Theorem asserts ‘a kind\\nof decidability,’ namely every quantificational formula is\\neither provable or a counterexample to it can be given, ‘the\\nprinciple of the excluded middle seems to express nothing other than\\nthe decidability of every problem’:  \\n… what is affirmed (by the law of excluded middle) is the\\nsolvability not at all through specified means but only through all\\nmeans that are …  \\nin any way imaginable  \\n[ ]  \\n20  \\nGödel considers intuitionistic propositional logic (henceforth\\nIPL); he also considers a second system, classical propositional logic\\nenriched by an operator “B”, where the intended meaning of\\n“B” is “provable.” The axiom system now known\\nas (for a list of these axioms see for example the\\nentry on in this encyclopedia) is added to the standard axioms for classical\\npropositional logic together with a new rule of proof: from , B may be inferred. Let us call this second\\nsystem . Gödel’s theorem states that is interpretable in via the following\\ntranslation:  \\nS4  \\nmodal logic  \\nA  \\nA  \\nG  \\nIPL  \\nG  \\n¬  \\np  \\n≡  \\n~B  \\np  \\n⊃  \\np  \\nq  \\n≡  \\nB → B  \\np  \\nq  \\n∨  \\np  \\nq  \\n≡  \\nB ∨ B  \\np  \\nq  \\n∧  \\np  \\nq  \\n≡  \\nB ∧ B  \\np  \\nq  \\nThat is,  \\n. Let be a formula of , and let ′\\nbe its translation. Then ⊢ implies ⊢ ′.  \\nTheorem 12  \\nA  \\nIPL  \\nA  \\nIPL  \\nA  \\nG  \\nA  \\nGödel conjectures that the converse implication must be true, and\\nindeed this was shown in McKinsey and Tarski 1948.  \\nThe difference between the two notions of provability: “provable\\nin a given formal system ” and provability by any\\ncorrect means — manifests itself as a consequence of\\nGödel’s Second Incompleteness Theorem, as follows. Let contain Peano arithmetic, and let the operator B be\\ninterpreted as “provable in ”. If the axioms of were valid for this interpretations of ,\\nthen from (0 ≠ 1) → (0 ≠ 1), the sentence\\n¬ (0 ≠ 1) would be provable, contradicting the Second\\nIncompleteness Theorem.  \\nS  \\nS  \\nS  \\nS4  \\nB  \\nB  \\nB  \\nFor further discussion of Gödel’s theorem, its antecedents\\nand its extensions, as well as its philosophical significance, the\\nreader is referred to A.S Troelstra’s introduction to .  \\n1933f  \\n2.5.4 Heyting Arithmetic is Interpretable into Computable Functionals of Finite Type.  \\nGödel’s so-called Dialectica intepretation (Gödel\\n1958) delivers a relative consistency proof and justification for\\nHeyting arithmetic by means of a concrete interpretation involving a\\nsystem of computable functionals of finite type. Taken\\ntogether with his 1933e, which reduces classical first order\\narithmetic to Heyting arithmetic, a justification in these terms is\\nalso obtained for classical first order arithmetic.  \\nT  \\nGödel’s inductive definition of the notion “function\\nof finite type” is as follows: (Gödel 1990, p. 245).  \\nThe functionals of type 0 are the natural numbers.  \\nIf ,…, are types and we have already defined\\nwhat functionals of types ,…, are,\\nthen ( ,…, ) is a type and a functional of that\\ntype assigns to every -tuple of functionals of respective\\ntypes ,…, , a functional of type .  \\nt  \\n0  \\nt  \\nk  \\nt  \\n0  \\nt  \\nk  \\nt  \\n0  \\nt  \\nk  \\nk  \\nt  \\n1  \\nt  \\nk  \\nt  \\n0  \\nGödel considers the quantifier free theory of these functionals\\nof finite type, denoted by . has the following\\nfeatures: the language of contains variables of each type,\\nconstants for distinguished types, and a ternary predicate\\n= for equality for type σ. Equality between\\nterms of the same type is decidable. The non-logical axioms and rules\\nfor include the classical arithmetic axioms for 0 and\\nsuccessor, and the induction rule:  \\nT  \\nT  \\nT  \\nσ  \\nT  \\n( (0) ∧ ( ( ) → ( ( )))) → ( )  \\nF  \\nF  \\nx  \\n0  \\nF  \\nS  \\nx  \\n0  \\nF  \\nx  \\n0  \\nfor quantifier-free formulas ( ). As\\nGödel remarks (Gödel 1990, p. 247), the axioms for are essentially those of primitive recursive arithmetic,\\nexcept that the variables can be of any finite type.  \\nF  \\nx  \\n0  \\nT  \\nGödel’s translation associates with every formula ( ) of the language of Peano arithmetic a\\nformula ′( ) =\\n∃ ∀ ( , , ) of the language of the theory , where is quantifier free and the (boldface)\\nbound variables are finite sequences of variables thought to range\\nover functionals of a finite type determined by the type of the\\nvariable. Intuitively, is a concrete analogue of\\nthe abstract notion of a construction constituting the meaning of .  \\nF  \\nx  \\nF  \\nx  \\ny  \\nz  \\nA  \\ny  \\nz  \\nx  \\nT  \\nA  \\ny  \\nF  \\nGödel’s theorem is as follows:  \\n. Suppose ′ =\\n∃ ∀ ( , , ). If is provable in\\nintuitionistic first order arithmetic, then there are computable\\nfunctionals of finite type such that ( ( ), , ) is provable in .  \\nTheorem 13  \\nF  \\ny  \\nz  \\nA  \\ny  \\nz  \\nx  \\nF  \\nQ  \\nA  \\nQ  \\nx  \\nz  \\nx  \\nT  \\nThe proof is by induction on the structure of the proof of in intuitionistic first order arithmetic. (For a treatment of the\\nproof in detail, the reader is referred to Troelstra 1986.)  \\nF  \\nThe importance of the theorem for foundations cannot be\\n overstated. A discussion of its generalizations, of ensuing work on functional\\ninterpretations stimulated by the theorem due to Kreisel, Tait,\\nHoward, Feferman and others; its foundational and philosophical\\nsignificance; and finally its relation particularly to the earlier,\\ninformal, proof interpretation, so-called, given by\\nHeyting-Kolmogorov, will not be attempted here. Accordingly the reader\\nis referred to the large literature on the subject, e.g., the\\nabovementioned Troelstra 1986, Tait 1967, Feferman 1993 and Avigad\\n& Feferman 1998. For interesting recent developments, e.g., in the\\narea of relating Gödel’s Dialectica interpretation and\\nKreisel’s modified realizability, see Oliva 2006. See also van\\nOosten 2008.  \\n[ ]  \\n21  \\nA remark concerning the philosophical context in which Gödel\\npresented his translation, namely finitism. The question addressed in\\nthe introduction to the paper is what notions must\\nbe added to finitary mathematics in order to obtain a consistency\\nproof for arithmetic. Equivalently: what does the finitary view\\npresuppose, which must be given up in the light of the Second\\nIncompleteness Theorem, if the consistency proof is to be\\nobtained:  \\nabstract  \\nIn any case Bernays’ remark teaches us to distinguish two\\ncomponents of the finitary attitude; namely, first, the constructive\\nelement, which consists in our being allowed to speak of mathematical\\nobjects only insofar as we can exhibit them or actually produce them\\nby means of a construction; second, the specifically finitistic\\nelement, which makes the further demand that the objects about which\\nwe make statements, with which the constructions are carried out and\\nwhich we obtain by means of these constructions, are\\n‘intuitive’, that is, are in the last analysis\\nspatiotemporal arrangements of elements whose characteristics other\\nthan their identity or nonidentity are irrelevant.… It is the\\nsecond requirement that must be dropped. This fact has hitherto been\\ntaken into account by our adjoining to finitary mathematics parts of\\nintuitionistic logic and the theory of ordinals. In what follows we\\nshall show that, for the consistency proof of number theory, we can\\nuse, instead, the notion of computable function of finite type on the\\nnatural numbers and certain rather elementary principles of\\nconstruction for such functions. (Gödel 1990, p.245).  \\nAside from its technical contribution, then, Gödel’s\\n1958/72 is one of Gödel’s most important philosophical\\nworks; notable for its analysis of the nature of finitary mathematics,\\nas well as its analysis of the notions of “intuitive,” as\\nin “intuitive knowledge,” and that of abstract versus\\nconcrete evidence.  \\nIn the next section, we turn to Gödel’s philosophical\\nviews. But interested readers may wish to read a brief discussion\\nabout Gödel’s Nachlass, important source of philosophical\\nmaterial by Gödel:  \\nSupplement Document: Gödel’s Documents  \\n3. Gödel’s Philosophical Views  \\nGödel’s philosophical views can be broadly characterized by\\ntwo points of focus, or, in modern parlance, commitments. These are:\\nrealism, namely the belief that mathematics is a descriptive science\\nin the way that the empirical sciences are. The second commitment is\\nto a form of Leibnizian rationalism in philosophy; and in fact\\nGödel’s principal philosophical influences, in this regard\\nparticularly but also many others, were Leibniz, Kant and Husserl.\\n(For further discussion of how these philosophers influenced\\nGödel, see van Atten and Kennedy 2003.)  \\nThe terms “Gödel’s realism” and\\n“Gödel’s rationalism” must be prefaced with a\\ndisclaimer: there is no single view one could associate with each of\\nthese terms. Gödel’s realism underwent a complex\\ndevelopment over time, in both the nature of its ontological claims as\\nwell as in Gödel’s level of commitment to those claims.\\nSimilarly Gödel’s rationalism underwent a complex\\ndevelopment over time, from a tentative version of it at the\\nbeginning, to what was adjudged to be a fairly strong version of it in\\nthe 1950s. Around 1959 and for some time afterward Gödel fused\\nhis rationalistic program of developing exact philosophy with the\\nphenomenological method as developed by Husserl.  \\nWe examine these two strains of Gödel’s thinking below:  \\n3.1 Gödel’s Rationalism  \\nGödel’s rationalism has its roots in the Leibnizian thought\\nthat the world, not that which we immanently experience but that which\\nitself gives rise to immanent experience, is perfect and beautiful,\\nand therefore rational and ordered. Gödel’s justification\\nof this belief rests partly on an inductive generalization from the\\nperfection and beauty of mathematics:  \\nRationalism is connected with Platonism because it is directed to the\\nconceptual aspect rather than toward the (real) world. One uses\\ninductive evidence…Mathematics has a form of\\nperfection…We may expect that the conceptual world is perfect,\\nand, furthermore, that objective reality is beautiful, good, and\\nperfect. (Wang 1996, 9.4.18)  \\nOur total reality and total experience are beautiful and\\nmeaningful—this is also a Leibnizian thought. We should judge\\nreality by the little which we truly know of it. Since that part which\\nconceptually we know fully turns out to be so beautiful, the real\\nworld of which we know so little should also be beautiful.\\n(9.4.20)  \\nAlthough the roots of Gödel’s belief in rationalism are\\nmetaphysical in nature, his long-standing aspirations in that domain\\nhad always been practical ones. Namely, to develop exact methods in\\nphilosophy; to transform it into an exact science, or , to use Husserl’s term.  \\nstrenge\\nWissenschaft  \\nWhat this means in practice is taking the strictest view possible of\\nwhat constitutes the grounds for the acceptance\\nof an assertion; put another way, a level of rigor is aspired to in\\nphilosophical arguments approaching that which is found in\\nmathematical proofs. A formulation of the view—one which is\\nsomewhat phenomenologically colored (see below)—can be found in\\na document in the Gödel Nachlass. This is a fourteen item list\\nGödel drew up in about 1960, entitled “My Philosophical\\nViewpoint.” Two items on the list are relevant here:  \\ndialectical  \\nThere are systematic methods for the solution of all problems\\n(also art, etc.).  \\nThere is a scientific (exact) philosophy and theology, which deals\\nwith concepts of the highest abstractness; and this is also most\\nhighly fruitful for science.  \\n(The list was transcribed by Cheryl Dawson and was published in , p. 316.)  \\nWang 1996  \\nGödel’s earlier conception of rationalism refers to\\nmathematical rigor and includes the concept of having a genuine proof,\\nand is therefore in some sense a more radical one than that to which\\nhe would later subscribe. One can see it at work at the end of the\\nGibbs lecture, after a sequence of arguments in favor of realism are\\ngiven:  \\nOf course I do not claim that the foregoing considerations amount to a\\nreal proof of this view about the nature of mathematics. The most I\\ncould assert would be to have disproved the nominalistic view, which\\nconsiders mathematics to consist solely in syntactical conventions and\\ntheir consequences. Moreover, I have adduced some strong arguments\\nagainst the more general view that mathematics is our own creation.\\nThere are, however, other alternatives to Platonism, in particular\\npsychologism and Aristotelian realism. In order to establish Platonic\\nrealism, these theories would have to be disproved one after the\\nother, and then it would have to be shown that they exhaust all\\npossibilities. I am not in a position to do this now; however I would\\nlike to give some indications along these lines. (Gödel 1995, p.\\n321–2).  \\n(For a penetrating analysis of this passage see Tait 2001.) Such an\\nanalysis must be based on conceptual analysis:  \\nI am under the impression that after sufficient clarification of the\\nconcepts in question it will be possible to conduct these discussions\\nwith mathematical rigour and that the result will then be…that\\nthe Platonistic view is the only one tenable. (Gödel 1995, p.\\n322).  \\nAlong with the methodological component, as can be seen from the items\\non Gödel’s list, there was also an “optimistic”\\ncomponent to Gödel’s rationalism: once the appropriate\\nmethods have been developed, philosophical problems such as, for\\nexample, those in ethics (e.g., item 9 on the list is: “Formal\\nrights comprise a real science.”) can be decisively solved. As\\nfor mathematical assertions, such as the Continuum Hypothesis in set\\ntheory, once conceptual analysis has been carried out in the right\\nway, that is, once the basic concepts, such as that of\\n“set,” have been completely clarified, the Continuum\\nHypothesis should be able to be decided.  \\nAlthough at the time of the Gibbs lecture the analogy in\\nGödel’s mind between philosophical and mathematical\\nreasoning may have been a very close one, Gödel’s view at\\nother periods was that the envisaged methods will not be mathematical\\nin nature. What was wanted was a general, informal science of\\nconceptual analysis.  \\nPhilosophy is more general than science. Already the theory of\\nconcepts is more general than mathematics…True philosophy is\\nprecise but not specialized.  \\nPerhaps the reason why no progress is made in mathematics (and there\\nare so many unsolved problems), is that one confines oneself to the\\next[ensional]—thence also the feeling of disappointment in the\\ncase of many theories, e.g., propositional logic and formalisation\\naltogether. (Wang 1996, 9.3.20,\\n 9.3.21)  \\n[ ]  \\n22  \\n(See notebook Max IV, p. 198 (Gödel Nachlaß, Firestone\\nLibrary, Princeton, item 030090). Transcription Cheryl Dawson;\\ntranslation from the German ours; amendment ours. Gödel’s\\ndating of Max IV indicates that it is from May 1941 to April 1942. See\\nalso Gödel’s letter to Bernays, Gödel 2003a, p.\\n283.)  \\nAn important source for understanding Gödel’s advance\\ntoward a general theory of concepts are Gödel’s remarks on\\nconceptual analysis published by Hao Wang in .\\nIn remark 8.6.10 for example, Gödel expresses the belief that\\nextensionality fails for concepts, contrary to what he said in his\\n1944 “Russell’s Mathematical Logic,” a remark which\\nhe now wishes to retract:  \\nLogical Journey  \\nI do not (no longer) believe that generally sameness of range is\\nsufficient to exclude the distinctness of two concepts.  \\nIn some of Gödel’s later discussions another component of\\nconceptual analysis emerges, namely the project of finding the\\nso-called primitive terms or concepts, and their relations. These are\\nroughly terms or concepts which comprise a theoretical “starting\\npoint,” on the basis of their meaning being completely definite\\nand clear. For example, the concept of “the application of a\\nconcept to another concept” is a primitive term, along with\\n“force”. (Wang 1996, 9.1.29).  \\nHe spoke to Wang about the general project in 1972:  \\nPhenomenology is not the only approach. Another approach is to find a\\nlist of the main categories (e.g., causation, substance, action) and\\ntheir interrelations, which, however, are to be arrived at\\nphenomenologically. The task must be done in the right manner. (Wang\\n1996, 5.3.7).  \\nGödel spoke with Sue Toledo between 1972 and 1975 about the\\nproject of finding primitive terms, as well as other aspects of\\nphenomenology. See Toledo 2011. We discuss Gödel’s\\ninvolvement with phenomenology further in the supplementary document .  \\nGödel’s Turn to Phenomenology  \\nThe judgement levied upon Gödel’s rationalism by\\ncontemporary philosophers was a harsh one. (See for example Gödel\\n1995, pp. 303–4). Nevertheless Gödel himself remained\\noptimistic. As he commented to Wang:  \\nIt is not appropriate to say that philosophy as rigorous science is\\nnot realizable in the foreseeable future. Time is not the main factor;\\nit can happen anytime when the right idea appears. (Wang 1996,\\n4.3.14).  \\nGödel concluded his 1944 on a similarly optimistic note.  \\n3.2 Gödel’s Realism  \\nGödel’s realist views were formulated mostly in the context\\nof the foundations of mathematics and set theory.  \\nWe referred above the list “What I believe,” thought to\\nhave been written in 1960 or thereabouts. Out of 14 items, only two\\nrefer to realism, remarks 10 and 12:  \\nMaterialism is false.  \\nConcepts have an objective existence.  \\nGödel published his views on realism for the first time in his\\n1944. The following is one of his most quoted passages on the\\nsubject:  \\nClasses and concepts may, however, also be conceived as real objects,\\nnamely classes as “pluralities of things,” or as\\nstructures consisting of a plurality of things and concepts as the\\nproperties and relations of things existing independently of our\\ndefinitions and constructions.  \\nIt seems to me that the assumption of such objects is quite as\\nlegitimate as the assumption of physical bodies and there is quite as\\nmuch reason to believe in their existence. They are in the same sense\\nnecessary to obtain a satisfactory system of mathematics as physical\\nbodies are necessary for a satisfactory theory of our sense\\nperceptions and in both cases it is impossible to interpret the\\npropositions one wants to assert about these entities as propositions\\nabout the “data,” i.e., in the latter case the actually\\noccurring sense perceptions.  \\nGödel’s reference to the impossibility of interpreting\\nempirical laws, or more precisely, instantiations of them—the\\nstatements “one wants to assert,”—as statements\\nabout sense perceptions, is likely an endorsement of the (then)\\ncontemporary critique of phenomenalism. The critique was based on the\\nobservation that sense data are so inextricably bound up with the\\nconditions under which they are experienced, that no correspondence\\nbetween statements about those and the statements “we want to\\nassert” can be given (see Chisholm 1948 for example). More\\ngenerally Gödel was against verificationism, namely the idea that\\nthe meaning of a statement is its mode of verification.  \\nThe analogical point in the first part of the passage was amplified by\\nGödel in the draft manuscript “Is Mathematics a Syntax of\\nLanguage?”:  \\nIt is arbitrary to consider “This is red” an immediate\\ndatum, but not so to consider the proposition expressing modus ponens\\nor complete induction (or perhaps some simpler propositions from which\\nthe latter follows). (Gödel 1995, p. 359)  \\nSome writers have interpreted Gödel in this and similar passages\\npragmatically, attributing to him the view that because empirical\\nstatements are paradigmatic of successful reference, reference in the\\ncase of abstract concepts should be modelled causally. (See Maddy\\n1990.) Interpreting reference to objects this way,\\nit is argued, addresses the main difficulty associated with realism,\\nthe problem how we can come to have knowledge of abstract objects.\\nOthers have argued that Gödel had no paradigm case in mind; that\\nfor him both the empirical and the abstract case are either equally\\nproblematic, or equally unproblematic. (See Tait 1986.) The latter\\nview is referred to as epistemological parity in van Atten and Kennedy\\n2003. (See also Kennedy and van Atten 2004.)  \\nabstract  \\nIn his 1947 “What is Cantor’s Continuum Problem?”,\\nGödel expounds the view that in the case of meaningful\\npropositions of mathematics, there is always a fact of the matter to\\nbe decided in a yes or no fashion. This is a direct consequence of\\nrealism, for if there exists a domain of mathematical objects or\\nconcepts, then any meaningful proposition concerning them must be\\neither true or\\n false. The Continuum Hypothesis is Gödel’s example of a\\nmeaningful question. The concept “how many” leads\\n“unambiguously” to a definite meaning of the hypothesis,\\nand therefore it should be decidable—at least in principle. Most\\nstrikingly Gödel does not leave the matter there but goes on to\\noffer a practical strategy for determining the value of the continuum,\\nas well as the truth value of other axioms extending .\\nSpecifically, he offers two criteria for their decidability: the first\\ninvolves conceptual analysis and is associated with Gödel’s\\nrationalistic program. (See the above section on Gödel’s\\nrationalism.) Secondly one must keep an eye on the so-called success\\nof the axiom, as a check or indicator of which direction to look to\\nfor the solution of its truth. For example, Gödel notes in the\\npaper that none of the consequences of the Axiom of Constructibility\\nare very plausible. It is, then, likely false. See Maddy 2011 and\\nKoellner 2014 for discussion of intrinsic vs extrinsic justifications\\nfor new axioms of set theory.  \\n[ ]  \\n23  \\nZFC  \\nFor further discussion of Gödel’s philosophical views see\\nthe supplementary documents:  \\nGödel’s Turn to Phenomenology  \\nand  \\nA Philosophical Argument About the Content of Mathematics  \\nBibliography  \\nPrimary Sources  \\nGödel’s Writings  \\nThe Gödel Nachlass is located at Firestone Library of Princeton\\nUniversity with the exception of Gödel’s preprint\\ncollection, which is housed at the library of the Institute for\\nAdvanced Study. The Nachlass itself is the property of the Institute\\nbut a microfilm copy of it may be purchased from Brill. All of\\nGödel’s published work, together with a large number of the\\nunpublished material from the Nachlass, together with a selection of\\nGödel’s correspondence is published in .  \\nKurt Gödel,\\nCollected Works, Volumes I-V  \\nThe Collected Papers of Kurt Gödel  \\n1986, .\\nS. Feferman, S. Kleene, G. Moore, R. Solovay, and J. van Heijenoort\\n(eds.), Oxford: Oxford University Press.  \\nCollected Works. I: Publications 1929–1936  \\n1990, .\\nS. Feferman, J. Dawson, S. Kleene, G. Moore, R. Solovay, and J. van\\nHeijenoort (eds.), Oxford: Oxford University Press.  \\nCollected Works. II: Publications 1938–1974  \\n1995, . S. Feferman, J. Dawson, S. Kleene, G. Moore, R.\\nSolovay, and J. van Heijenoort (eds.), Oxford: Oxford University\\nPress.  \\nCollected Works. III: Unpublished essays and\\nlectures  \\n2003a, . S.\\nFeferman, J. Dawson, S. Kleene, G. Moore, R. Solovay, and J. van\\nHeijenoort (eds.), Oxford: Oxford University Press.  \\nCollected Works. IV: Correspondence A-G  \\n2003b, . S.\\nFeferman, J. Dawson, S. Kleene, G. Moore, R. Solovay, and J. van\\nHeijenoort (eds.), Oxford: Oxford University Press.  \\nCollected Works. V: Correspondence H-Z  \\nSelected Works of Kurt Gödel  \\n[1929]  \\n“I”, . Reprinted in Gödel 1986, pp. 60–101.  \\nDissertation, University of\\nVienna  \\n[1930]  \\n“Die Vollständigkeit der Axiome des\\nlogischen Funktionenkalküls”, , 37: 349–360. Reprinted in Gödel\\n1986, pp. 102–123.  \\nMonatshefte für\\nMathematik und Physik  \\n[1931]  \\n“Über formal unentscheidbare Sätze\\nder Principia Mathematica und verwandter Systeme, I”, , 38:\\n173–198. Reprinted in Gödel 1986, pp. 144–195.  \\nMonatshefte für Mathematik und Physik  \\n[1932]  \\n“Zum intuitionistischen\\nAussagenkalkül”, , 69: 65–66. Reprinted in Gödel\\n1986, pp. 222–225.  \\nAnzeiger der Akademie der\\nWissenschaften in Wien  \\n[1933e]  \\n“Zur intuitionistischen Arithmetik und\\nZahlentheorie”, , 4: 34–38. Reprinted in Gödel 1986, pp.\\n286–295.  \\nErgebnisse eines mathematischen\\nKolloquiums  \\n[1933f]  \\n“Eine Interpretation des intuitionistischen\\nAussagenkalküls”, 4, 39–40. Reprinted in Gödel 1986, pp.\\n300–301.  \\nErgebnisse eines mathematischen\\nKolloquiums  \\n[1933i]  \\n“Zum Entscheidungsproblem des logischen\\nFunctionenkalküls”, , 40: 433–443. Reprinted in Gödel 1986, pp.\\n306–326.  \\nMonatshefte für Mathematik und\\nPhysik  \\n[*1933o]  \\n“The present situation in the foundations of\\nmathematics”, manuscript. Printed in Gödel 1995, pp.\\n45–53.  \\n[1934c]  \\nReview of Skolem (1933). , 7: 193–194. Reprinted in\\nGödel 1986, pp. 379–380.  \\nZentralblatt für\\nMathematik und ihre Grenzgebiete  \\n[1936a]  \\n“Über die Länge von\\nBeweisen”, ,\\n7: 23–24. Reprinted in Gödel 1986, pp. 395–399.  \\nErgebnisse eines mathematischen Kolloquiums  \\n[1939a]  \\n“Consistency proof for the generalized\\ncontinuum hypothesis”, , 25: 220–224. Reprinted in Gödel\\n1990, pp. 28–32.  \\nProceedings of the National Academy\\nof Sciences, U.S.A.  \\n[1940]  \\n“The Consistency of the Continuum\\nHypothesis”, , Volume 3,\\nPrinceton: Princeton University Press. Reprinted in Gödel 1990,\\npp. 33–101.  \\nAnnals of Mathematics Studies  \\n[*1941]  \\n“In what sense is intuitionistic logic\\nconstructive?”, lecture manuscript. Printed in Gödel 1995,\\npp. 189–200.  \\n[1944]  \\n“Russell’s mathematical logic”, (Library of Living\\nPhilosophers), P. Schilpp (ed.), New York: Tudor, 1951, pp.\\n123–153. Reprinted in Gödel 1990, pp. 119–141.  \\nThe Philosophy of Bertrand Russell  \\n[*1946/9-B2]  \\n“Some observations about the relationship\\nbetween theory of relativity and Kantian philosophy”,\\nmanuscript. Printed in Gödel 1995, pp. 230–246.  \\n[*1946/9-C1]  \\n“Some observations about the relationship\\nbetween theory of relativity and Kantian philosophy”,\\nmanuscript. Printed in Gödel 1995, pp. 247–259.  \\n[1947]  \\n“What is Cantor’s continuum\\nproblem?”, , 54: 515–525.\\nReprinted in Gödel 1990, pp. 176–187.  \\nAmer. Math. Monthly  \\n[1949a]  \\n“A remark on the relationship between\\nrelativity theory and idealistic philosophy”, (Library of Living Philosophers),\\nP. Schilpp (ed.), La Salle, IL: Open Court, 1949, pp. 555–562.\\nReprinted in Gödel 1990, pp. 202–207.  \\nAlbert\\nEinstein: Philosopher-Scientist  \\n[1949]  \\n“An Example of a New Type of Cosmological\\nSolutions of Einstein’s Field Equations of Gravitation,” , 21: 447–450. Reprinted in\\nGödel 1990, pp. 190–198.  \\nReviews of Modern Physics  \\n[*1951]  \\n“Some basic theorems on the foundations of\\nmathematics and their implications”, lecture manuscript. Printed\\nin Gödel 1995, pp. 304–323.  \\n[*1953/9-III]  \\n“Is mathematics a syntax of language?”,\\nlecture manuscript. Printed in Gödel 1995, pp.\\n334–356.  \\n[*1953/9-V]  \\n“Is mathematics a syntax of language?,”\\nlecture manuscript. Printed in Gödel 1995, pp.\\n356–362.  \\n[1958]  \\n“Über eine bisher noch nicht\\nbenützte Erweiterung des finiten Standpunktes”, , 12: 280–287. Reprinted in Gödel 1990,\\npp. 240–251.  \\nDialectica  \\n[*1961/?]  \\n“The modern development of the foundations of\\nmathematics in light of philosophy”, manuscript. Printed in\\nGödel 1995, pp. 374–387.  \\n[1964]  \\n“What is Cantor’s continuum\\nproblem? , revised version of Gödel 1947, in\\nBenacerraf, P. and Putnam, H. (eds.), 1983, , Cambridge: Cambridge\\nUniversity Press. Reprinted in Gödel 1990, pp.\\n254–270.  \\n”  \\nPhilosophy of\\nmathematics: selected readings (2nd ed.)  \\n[*1970]  \\n“Ontological proof”, manuscript.\\nPrinted in Gödel 1995, pp. 403–404.  \\n[*1970a]  \\n“Some considerations leading to the probable\\nconclusion that the true power of the continuum is\\nℵ ”, manuscript. Printed in Gödel 1995,\\npp. 420–422.  \\n2  \\n[*1970b]  \\n“A proof of Cantor’s continuum\\nhypothesis from a highly plausible axioms about orders of\\ngrowth”, manuscript. Printed in Gödel 1995, pp.\\n422–423.  \\nSecondary Sources  \\nAvigad, J. and S. Feferman, 1998, “Gödel’s\\nFunctional (‘Dialectica’) Interpretation”, in (Studies in Logic and the\\nFoundations of Mathematics, Volume 137), Samuel Buss (ed.), Amsterdam:\\nNorth-Holland, pp. 337-405.  \\nHandbook of Proof Theory  \\nAwodey, S. and A. W. Carus, 2010, “Gödel and\\nCarnap”, in ,\\nSolomon Feferman, Charles Parsons & Stephen G. Simpson (eds.),\\nCambridge: Cambridge University Press.  \\nKurt Gödel: Essays for his Centennial  \\nBaaz, M., and C. Papadimitriou, D.Scott, H. Putnam, and C. Harper\\n(eds.), 2011, , Cambridge: Cambridge University Press.  \\nKurt Gödel and the Foundations of Mathematics:\\nHorizons of Truth  \\nBadesa, C., and P. Mancosu, and R. Zach, 2009, “The\\nDevelopment of Mathematical Logic from Russell to Tarski,\\n1900–1935”, in Leila Haaparanta (ed.), . New York and Oxford: Oxford University\\nPress:318–470..  \\nThe History of\\nModern Logic  \\nBarwise, Jon (ed.), 1977, (Studies in Logic and the Foundations of Mathematics, Volume 90),\\nAmsterdam: North-Holland Publishing Co.  \\nHandbook of Mathematical Logic  \\nBehmann, Heinrich, 1922, “Beiträge, Algebra, Logik,\\ninsbesodere zum Entscheidungsproblem”, , 86: 419–432.  \\nMathematische\\nAnnalen  \\nBenacerraf, P. and H. Putnam (eds.), 1983, , Cambridge: Cambridge University\\nPress, 2nd edition.  \\nPhilosophy of\\nMathematics: Selected Readings  \\nBernays, Paul, 1926, “Axiomatische Untersuchung des\\nAussagen-Kalkuls der ‘Principia Mathematica’”, , 25(1): 305–320.  \\nMathematisches Zeitschrift  \\nBezboruah, A., and J.C. Sheperdson, 1976,\\n“Gödel’s second incompleteness theorem for\\n\\\\(Q\\\\)”, , 41 (2):\\n503–512.  \\nJournal of Symbolic Logic  \\nBolzano, Bernard, 1969, , Sections\\n349–391, in ,\\nReihe I/Band 13, edited and with an introduction by Jan Berg,\\nStuttgart-Bad Cannstatt: Frommann Holzboog.  \\nWissenschaftslehre  \\nBernard Bolzano — Gesamtausgabe  \\nBurgess, John, 2009, “”Intuitions of Three Kinds in\\nGödel’s Views on the Continuum“”, in Kennedy, J. (ed.) Cambridge: Cambridge\\nUniversity Press, 2014.  \\nInterpreting Gödel  \\nBuss, Samuel R., 1994, “On Gödel’s Theorems on\\nLengths of Proofs. I. Number of Lines and Speedup for\\nArithmetics”, , 59(3):\\n737–756.  \\nJournal of Symbolic Logic  \\nChisholm, R., 1948, “The Problem of Empiricism”, , 45: 512–7.  \\nThe Journal of Philosophy  \\nCohen, Paul, 1963, “The Independence of the Continuum\\nHypothesis”, , 50: 1143–1148.  \\nProceedings of the National Academy of Sciences\\nof the U.S.A.  \\nCrocco, G., 2003, “Gödel, Carnap and the Fregean\\nHeritage”, , 27:\\n171–191.  \\nHistory and Philosophy of Logic  \\n–––, 2006, “Gödel on Concepts”, , 137(1,2): 21–41.  \\nSynthese  \\nDawson, Jr., John W., 1997, , Wellesley, MA: A. K. Peters, Ltd.  \\nLogical dilemmas: The Life and\\nWork of Kurt Gödel  \\nDehornoy, Patrick, 2004, “Progrès récents sur\\nl’hypothèse du continu (d’après\\nWoodin)”, , 294: viii,\\n147–172.  \\nAstérisque  \\nDetlefsen, Michael, 1986, , Dordrecht: D. Reidel.  \\nHilbert’s Program: An essay on\\nmathematical instrumentalism  \\n–––, 2001, “What Does Gödel’s\\nSecond theorem Say?”, , 9(1):\\n37–71.  \\nPhilosophia Mathemathica  \\n–––, 2014, “Completeness and the Ends of\\nAxiomatization”, in , Kennedy,\\nJ. (ed.) Cambridge: Cambridge University Press, 2014.  \\nInterpreting Gödel  \\nDreben, B. and J. van Heijenoort, 1986, “Introductory Note\\nto 1929, 1930 and 1930a”, in Gödel 1986, pp.\\n44–59.  \\nEdwards, Paul (ed.), 1967, , New York: MacMillan.  \\nThe Encyclopedia of\\nPhilosophy  \\nEhrenfeucht, A. and J. Mycielski, 1971, “Abbreviating Proofs\\nby Adding New Axioms”, , 77: 366–367.  \\nBulletin of the American Mathematical\\nSociety  \\nFeferman, Solomon, 1960/1961, “Arithmetization of\\nMetamathematics in a General Setting”, , 49: 35–92.  \\nFundamenta\\nMathematicae  \\n–––, 1993, “Gödel’s Dialectica\\nInterpretation and Its Two-way Stretch”, in (Lecture Notes in Computer Science, Volume\\n713), G. Gottlob, A. Leitsch, and D. Mundici (eds.), Berlin: Springer,\\npp. 23–40.  \\nComputational\\nLogic and Proof Theory  \\n–––, 1986, “Gödel’s Life and\\nWork”, in Gödel 1986, pp. 1–34.  \\n–––, 1988, “Hilbert’s Program\\nRelativized: Proof-Theoretical and Foundational Reductions”, , 53: 364–384.  \\nJournal of Symbolic Logic  \\n–––, 1996, “Proof Theory”, in , D. Borchrt (ed.),\\nNew York: MacMillan, pp. 466–469.  \\nThe Encyclopedia of Philosophy Supplement  \\nFeferman, S., and H. Friedman, P. Maddy, and J. Steel, 2000,\\n“Does Mathematics Need New Axioms?”, , 6(4): 401–446.  \\nBulletin of\\nSymbolic Logic  \\nFeferman, S., C. Parsons, and S. Simpson (eds.), 2010, (Lecture Notes in Logic,\\n33), Cambridge: Cambridge University Press.  \\nKurt\\nGödel: Essays for his Centennial  \\nFeigl, H. and A. Blumberg, 1931, “Logical Positivism. A New\\nMovement in European Philosophy”, , 28: 281–296.  \\nJournal of\\nPhilosophy  \\nFloyd, J. and A. Kanamori, 2006, “How Gödel Transformed\\nSet Theory”, , 53(4): 419–427.  \\nNotices of the American Mathematical\\nSociety  \\nFolina, Janet, 2014, “Gödel on How to Have your\\nMathematics and Know it Too”, in , Kennedy, J. (ed.) Cambridge: Cambridge University\\nPress, 2014.  \\nInterpreting\\nGödel  \\nFøllesdal, Dagfinn, 1995, “Gödel and\\nHusserl”, in (Synthese\\nLibrary, Volume 251), J. Hintikka (ed.), Dordrecht, Boston: Kluwer,\\npp. 427–446.  \\nFrom Dedekind to Gödel  \\nForeman, Matthew, 1998, “Generic Large Cardinals: New Axioms\\nfor Mathematics?”, in , Extra\\nVolume, Proceedings of the International Congress of Mathematicians,\\nII, pp. 11–21\\n [ (in compressed Postscript)].  \\nDocumenta Mathematica  \\navailable online  \\nFranks, Curtis, 2009, “The Autonomy of Mathematical\\nKnowledge: Hilbert’s Program Revisited”, Cambridge:\\nCambridge University Press.  \\n–––, 2011, “Stanley Tennenbaum’s\\nSocrates”, in , Kennedy, J. and Kossak, R.,\\n(eds.), Lecture Notes in Logic, 36, Cambridge: Cambridge University\\nPress, 2011.  \\nSet Theory, Arithmetic and Foundations of\\nMathematics: Theorems, Philosophies  \\n–––, 2014, “Logical Completeness, Form and\\nContent: An Archaeology”, in ,\\nKennedy, J. (ed.) Cambridge: Cambridge University Press, 2014.  \\nInterpreting Gödel  \\nGaifman, H., 2000, “What Godel’s Incompleteness Result\\nDoes and Does Not Show”, , 97 (8):\\n462–471.  \\nJournal of Philosophy  \\nGarson, James, 2003, “Modal Logic”, in , Fall 2003 Edition, Edward N.\\nZalta (ed.), URL =\\n < >.  \\nThe\\nStanford Encyclopedia of Philosophy  \\nhttps://plato.stanford.edu/archives/fall2003/entries/logic-modal/  \\nGlivenko, V., 1929, “Sur quelques points de la logique de m.\\nBrouwer.”, , 15: 183–188.  \\nAcadémie Royale de Belgique, Bulletin de\\nla Classe des Sciences  \\nGottwald, Siegfried, 2004, “Many-valued Logic”, in , Winter 2004 Edition,\\nEdward N. Zalta (ed.), URL =\\n < >.  \\nThe Stanford Encyclopedia of Philosophy  \\nhttps://plato.stanford.edu/archives/win2004/entries/logic-manyvalued/  \\nGödel, Rudolf, 1983, “History of the Gödel\\nFamily”, Susan Simonsin (trans.), in Weingartner and Schmetterer\\n1987, pp. 11–27.  \\nHauser, Kai, 2006, “Gödel’s Program Revisited,\\nPart 1: the Turn to Phenomenology”, , 12 (4): 529–590.  \\nBulletin of Symbolic\\nLogic  \\nHeyting, Arendt, 1930, “Die formalen Regeln der\\nintuitionistischen Logik”, ,\\nII, pp. 42–56.  \\nSitzungsberichte der Preussischen\\nAkademie der Wissenschaften, physikalisch-mathematische Klasse  \\nHilbert, David, 1926, “Über das Unendliche”, , 95: 161–190.  \\nMathematische Annalen  \\nHilbert, D. and W. Ackermann, 1928, , Berlin: Springer-Verlag.  \\nGrundzüge der\\ntheoretischen Logik  \\nHilbert, D. and P. Bernays, 1934, , Volume 1, Berlin: Springer-Verlag.  \\nGrundlagen der\\nMathematik  \\n–––, 1939, ,\\nVolume II, Berlin: Springer-Verlag.  \\nGrundlagen der Mathematik  \\nHodges, Wilfrid, 2005, “Model Theory”, in , Fall 2005 Edition, Edward N.\\nZalta (ed.), URL =\\n < >.  \\nThe\\nStanford Encyclopedia of Philosophy  \\nhttps://plato.stanford.edu/archives/fall2005/entries/model-theory/  \\nHusserl, Edmund, 1911, “Philosophie als strenge\\nWissenschaft”, , 1: 289–341.  \\nLogos  \\nJaśkowski, Stanisław, 1936, “Investigations into\\nthe System of Intuitionist Logic”, , 34(2)\\n(1975): 117–120. (Translated by S. McCall from the French\\n“Rechereches sur le système de la logique\\nintuitioniste” in , Volume VI, Hermann, Paris, 1936, pp.\\n58–61.)  \\nStudia Logica  \\nActes du Congrés International de\\nPhilosophie Scientifique  \\nJech, Thomas, 2003, , (Springer Monographs in\\nMathematics), Berlin: Springer-Verlag. 3rd millennium edition, revised\\nand expanded.  \\nSet theory  \\nJensen, R. Björn, 1972, “The Fine Structure of the\\nConstructible Hierarchy” (with a section by Jack Silver), , 4: 229–308; Erratum, 4\\n(1972): 443.  \\nAnnals of Mathematical Logic  \\nKanamori, Aki, 1996, “The Mathematical Development of Set\\ntheory from Cantor to Cohen.” , 2(1): 1–71.  \\nBulletin of Symbolic\\nLogic  \\n–––, 2006, “Levy and Set Theory”, , 140(3): 233–252.  \\nAnnals of Pure and Applied Logic  \\nKennedy, Juliette, 2006, “Incompleteness — A Book\\nReview,” ,\\n53(4): 448–455.  \\nNotices of the American Mathematical Societ  \\n–––, 2011, “Gödel’s Thesis: An\\nAppreciation” in , M. Baaz, C. Papadimitriou, D.\\nScott, H. Putnam, and C. Harper (eds.), Cambridge: Cambridge\\nUniversity Press 95–110.  \\nKurt Gödel and the Foundations of\\nMathematics: Horizons of Truth  \\n–––, 2013, “On Formalism Freeness:\\nImplementing Gödel’s 1946 Princeton Bicentennial\\nLecture”, , 19(3):\\n351–393.  \\nBulletin of Symbolic Logic  \\n–––, 2014, “Gödel’s 1946\\nPrinceton Bicentennial Lecture: An Appreciation”, in , Cambridge:\\nCambridge University Press.  \\nInterpreting Gödel: Critical Essays  \\nKennedy, Juliette (ed.), 2014, , Cambridge: Cambridge University Press.  \\nInterpreting Gödel:\\nCritical Essays  \\nKennedy, J. and van Atten, M., 2003, “On the Philosophical\\nDevelopment of Kurt Gödel”, , 9(4): 425–476. Reprinted in , Solomon Feferman, Charles Parsons and\\nStephen G. Simpson (eds.), Cambridge: Cambridge University Press.  \\nBulletin of Symbolic\\nLogic  \\nKurt Gödel:\\nEssays for his Centennial  \\n–––, 2004, “Gödel’s Modernism:\\nOn Set-theoretic Incompleteness”, , 25(2): 289–349. (See the Erratum in , 26(1) (2005), page\\nfacing contents.)  \\nGraduate Faculty\\nPhilosophy Journal  \\nGraduate Faculty Philosophy Journal  \\n–––, 2009, “Gödel’s Modernism:\\nOn Set-theoretic Incompleteness, Revisited”, in , S.\\nLinström, E. Palmgren, K. Segerberg, and V. Stoltenberg-Hansen\\n(eds.), Berlin: Springer: 303–356.  \\nLogicism,\\nIntuitionism and Formalism: What has become of them?  \\n–––, 2009, “Gödel’s\\nLogic”, in D. Gabbay and J. Woods (eds.), , Volume 5,\\nAmsterdam: Elsevier: 449–509.  \\nThe Handbook of\\nthe History of Logic: Logic from Russell to Gödel  \\nKleene, S. C., 1987, “Gödel’s Impression on\\nStudents of Logic in the 1930s”, in Weingartner and Schmetterer\\n1987, pp. 49–64.  \\nKoellner, Peter, 2014, “Large Cardinals and\\nDeterminacy”, (Spring Edition), Edward N. Zalta (ed.), URL =\\n < >.  \\nThe Stanford Encyclopedia of Philosophy  \\nhttps://plato.stanford.edu/archives/spr2014/entries/large-cardinals-determinacy/  \\nKreisel, Georg, 1980, “Kurt Gödel, 28 April 1906\\n– 14 January 1978”, , 26: 148–224. Corrigenda, 27 (1981): 697;\\nfurther corrigenda, 28 (1982): 697.  \\nBiographical Memoirs of Fellows of\\nthe Royal Society  \\n–––, 1988, “Review of Kurt Gödel: , Volume I”, , 29(1): 160–181.  \\nCollected works  \\nNotre Dame Journal of\\nFormal Logic  \\n–––, 1990, “Review of Kurt Gödel: , Volume II”, , 31(4): 602–641.  \\nCollected Works  \\nNotre Dame Journal of\\nFormal Logic  \\n–––, 1998, “Second Thoughts Around Some of\\nGödel’s Writings: A Non-academic Option”, , 114(1): 99–160.  \\nSynthese  \\nKripke, Saul, 2009, “The collapse of the Hilbert program:\\nwhy a system cannot prove its own 1-consistency”, , 15 (2): 229–231.  \\nBulletin\\nof Symbolic Logic  \\nKunen, Kenneth, 1983, , (Studies in Logic and the Foundations of\\nMathematics, Volume 102), Amsterdam: North-Holland Publishing Co.\\nReprint of the 1980 original.  \\nSet Theory: An Introduction to\\nIndependence Proofs  \\nLöb, M. H., 1956, “Formal Systems of Constructive\\nMathematics”, , 21:\\n63–75.  \\nJournal of Symbolic Logic  \\nLöwenheim, L., 1915, “Über Möglichkeiten im\\nRelativkalkül”, , 76(4):\\n447–470.  \\nMathematische Annalen  \\nŁukasiewicz, Jan, 1970, , (Studies in\\nLogic and the Foundations of Mathematics), L. Borkowski (ed.),\\nAmsterdam: North-Holland Publishing Co.  \\nSelected works  \\nMaddy, Penelope, 1990, , New York:\\nClarendon Press.  \\nRealism in Mathematics  \\nMaddy, Penelope, 2011, , Oxford:\\nOxford University Press.  \\nDefending the Axioms  \\nMal’cev, Anatoli Ivanovic, 1971, (Studies in\\nLogic and the Foundations of Mathematics, Volume 66), translated,\\nedited, and provided with supplementary notes by Benjamin Franklin\\nWells, III, Amsterdam: North-Holland Publishing Co.  \\nThe Metamathematics of\\nAlgebraic Systems. Collected Papers: 1936–1967  \\nMancosu, Paolo, 1998, , Oxford: Oxford\\nUniversity Press.  \\nFrom Brouwer to Hilbert. The Debate on\\nthe Foundations of Mathematics in the 1920s  \\n–––, 2004, “Review of Kurt Gödel, , Volumes IV and V”, , 45: 109–125.  \\nCollected Works  \\nNotre Dame\\nJournal of Formal Logic  \\nMartin, D.A., 2005, “Gödel’s Conceptual\\nRealism”, , 11:\\n207–224.  \\nBulletin of Symbolic Logic  \\nMcKinsey, J. C. C. and A. Tarski, 1948, “Some Theorems About\\nthe Sentential Calculi of Lewis and Heyting”, , 13: 1–15.  \\nJournal of\\nSymbolic Logic  \\nMostowski, Andrzej, 1949, “An Undecidable Arithmetical\\nStatement”, , 36:\\n143–164.  \\nFundamenta Mathematicae  \\n–––, 1982, , Westport, CT: Greenwood Press. Reprint of the 1952\\noriginal.  \\nSentences Undecidable in\\nFormalized Arithmetic: An Exposition of the Theory of Kurt\\nGödel  \\nOliva, Paulo, 2006, “Unifying Functional\\nInterpretations”, ,\\n47(2): 263–290.  \\nNotre Dame Journal of Formal Logic  \\nParikh, Rohit, 1971, “Existence and Feasibility in\\nArithmetic”, , 36:\\n494–508.  \\nJournal of Symbolic Logic  \\nParsons, Charles, 1995a, “Platonism and Mathematical\\nIntuition in Kurt Gödel’s Thought”, , 1(1): 44–74.  \\nBulletin of\\nSymbolic Logic  \\n–––, 1995b, “Quine and Gödel on\\nAnalyticity”, in , Cambridge:\\nCambridge University Press, pp. 297–313.  \\nOn Quine: New Essays  \\n–––, 2000, “Reason and Intuition”, , 125(3): 299–315.  \\nSynthese  \\n–––, 2002, “Realism and the Debate on\\nImpredicativity, 1917–1944”, in ,\\n(Lecture Notes in Logic, Volume 15), W. Sieg, R. Sommer, and C.\\nTalcott (eds.), Urbana, IL: Association of Symbolic Logic, pp.\\n372–389.  \\nReflections on the\\nFoundations of Mathematics: Essays in Honor of Solomon Feferman  \\n–––, 2010, “Gödel and Philosophical\\nIdealism” , 18 (2):\\n166–192.  \\nPhilosophia Mathematica  \\n–––, 2014, “Analyticity for\\nRealists”, in , Kennedy, J.\\n(ed.) Cambridge: Cambridge University Press, 2014.  \\nInterpreting Gödel  \\nPoonen, Bjorn, 2014, “Undecidable Problems: A\\nSampler”, in ,\\nCambridge: Cambridge University Press.  \\nInterpreting Gödel: Critical Essays  \\nPost, Emil L., 1921, “Introduction to a General Theory of\\nElementary Propositions”, , 43(3): 163–185.  \\nAmerican Journal of\\nMathematics  \\nPudlák, Pavel, 1996, “On the lengths of proofs of\\nconsistency: a survey of results”, , 2: 65-86.  \\nAnnals of the Kurt\\nGödel Society  \\nRaatikainen, P., 2005, “On the Philosophical Relevance of\\nGödel’s Incompleteness Theorems”, , 59 (4): 513–534.  \\nRevue\\nInternationale de Philosophie  \\nRogers, Jr., Hartley, 1967, , New York: McGraw-Hill Book Co.  \\nTheory of Recursive Functions and\\nEffective Computability  \\nRosser, J.B., 1936, “Extensions of Some Theorems of\\nGödel and Church”, ,\\n1(3): 87–91.  \\nJournal of Symbolic Logic  \\nScott, Dana, 1961, “Measurable Cardinals and Constructible\\nSets”, (Série des Science, Mathématiques,\\nAstronomiques et Physiques), 9: 521–524.  \\nBulleint de l’Academie Polonaise des\\nSciences  \\nShelah, Saharon, 2014, “Reflecting on Logical Dreams”,\\nin , Cambridge:\\nCambridge University Press.  \\nInterpreting Gödel: Critical Essays  \\nSieg, Wilfried, 1988, “Hilbert’s Program Sixty Years\\nLater”, , 53(2):\\n338–348.  \\nJournal of Symbolic Logic  \\n–––, 1990, “Relative Consistency and\\nAccessible Domains”, , 84(2):\\n259–297.  \\nSynthese  \\n–––, 1999, “Hilbert’s Programs:\\n1917–1922”, , 5(1):\\n1–44.  \\nBulletin of Symbolic Logic  \\n–––, 2006, “Gödel on\\nComputability”, , 14:\\n189–207.  \\nPhilosophia Mathematica  \\nSierpinski, Wacław, 1947, “L’hypothèse\\ngénéralisée du continu et l’axiome du\\nchoix”, , 34: 1–5.  \\nFundamenta Mathematicae  \\nSigmund, Karl, 2006, “Pictures at an Exhibition”, , 53(4):\\n428–432.  \\nNotices of the American Mathematical Society  \\nSkolem, Thoralf, 1920, “Logisch-kombinatorische\\nUntersuchungen über die Erfüllbarkeit oder Beweisbarkeit\\nmathematischer Sätze nebst einem Theoreme über dichte\\nMengen”, , I. ,\\nNumber 4, pp. 1–36. Reprinted in Skolem 1970, pp.\\n103–136.  \\nSkrifter utgit av Videnskappsselskapet i\\nKristiania  \\nMatematisk-naturvidenskabelig klasse  \\n–––, 1923, “Einige Bemerkungen zur\\naxiomatischen Begründung der Mengenlehre”, ,\\nHelsinki, pp. 217–232. Reprinted in Skolem 1970, pp.\\n137–152.  \\nMatematikerkongressen i Helsingfors den 4–7 Juli 1922, Den\\nfemte skandinaviska matematikerkongressen, Redogörelse  \\n–––, 1933, “Über die\\nUnmöglichkeit einer vollständigen Charakterisierung der\\nZahlenreihe mittels eines endlichen Axiomensystems”, , 10: 73–82.  \\nNorsk\\nMatematisk forenings skrifter  \\n–––, 1970, ,\\nJens Erik Fenstad (ed.), Oslo: Universitetsforlaget.  \\nSelected Works in Logic  \\nSmith, David Woodruff, 2005, “Phenomenology”, in (Winter Edition),\\nEdward N. Zalta (ed.), URL =\\n < >.  \\nThe Stanford Encyclopedia of Philosophy  \\nhttps://plato.stanford.edu/archives/win2005/entries/phenomenology/  \\nSolovay, Robert, 1990, “Introductory Note to 1938, 1939,\\n1939a, 1940”, in Gödel 1990, pp. 1–25.  \\nSteel, John, 2000, “Mathematics Needs New Axioms”, , 6(4): 422–433.  \\nBulletin of Symbolic Logic  \\nSteel, John, 2014, “Gödel’s Program”, in , Kennedy, J. (ed.) Cambridge:\\nCambridge University Press, 2014.  \\nInterpreting Gödel  \\nTait, William, 1967, “Intensional Interpretations of\\nFunctionals of Finite Type I,” , 32(2): 198–212.  \\nJournal of Symbolic\\nLogic  \\n–––, 1981, “Finitism”, , 78: 524–556. Reprinted in Tait 2005, pp.\\n21–42.  \\nJournal\\nof Philosophy  \\n–––, 1986, “Truth and Proof: The Platonism\\nof Mathematics”, , 69(3): 341–370.\\nReprinted in Tait 2005, pp. 61–88.  \\nSynthese  \\n–––, 2001, “Gödel’s Unpublished\\nPapers on Foundations of Mathematics”, , 9(1): 87–126. Reprinted in Tait 2005, pp.\\n276–313.  \\nPhilosophia\\nMathematica  \\n–––, 2002, “Remarks on Finitism”, in (Lecture Notes in Logic, Volume 15), W. Sieg, R.\\nSommer, and C. Talcott (eds.), Urbana, IL: Association of Symbolic\\nLogic, pp. 410–419. Reprinted in Tait 2005, pp.\\n43–53.  \\nReflections on the Foundations of Mathematics: Essays in Honor of\\nSolomon Feferman  \\n–––, 2005, (Logic\\nand Computation in Philosophy), New York: Oxford University\\nPress.  \\nThe Provenance of Pure Reason:\\nEssays in the Philosophy of Mathematics and its History  \\n–––, 2006, “Gödel’s\\ncorrespondence on proof theory and constructive\\nmathematics” , 14 (1):\\n76–111.  \\nPhilosophia Mathematica  \\n–––, 2006, “Gödel’s\\ninterpretation of intuitionism”, , 14 (2): 208–228.  \\nPhilosophia\\nMathematica  \\nTaussky-Todd, Olga, 1983, “Remembrances of Kurt\\nGödel”, in Weingartner and Schmetterer 1987, pp.\\n29–41.  \\nTieszen, Richard, 1992, “Kurt Gödel and\\nPhenomenology”, , 59(2):\\n176–194.  \\nPhilosophy of Science  \\n–––, 2002, “Gödel and the Intuition\\nof Concepts”, , 133 (3): 363–391.  \\nSynthese  \\n–––, 2005, , Cambridge: Cambridge University\\nPress.  \\nPhenomenology, Logic and the\\nPhilosophy of Mathematics  \\n–––, 2011, , Oxford: Oxford University\\nPress.  \\nAfter Gödel: Platonism and\\nRationalism in Mathematics and Logic  \\nToledo, Sue, 2011, “Sue Toledo’s Notes of her\\nConversations with Kurt Gödel in 1972-5”, in (Lecture Notes in Logic, 36), Kennedy, J. and\\nKossak, R., (eds.), Cambridge: Cambridge University Press,\\nforthcoming.  \\nSet\\nTheory, Arithmetic and Foundations of Mathematics: Theorems,\\nPhilosophies  \\nTragesser, Robert, 1977, , Ithaca:\\nCornell University Press.  \\nPhenomenology and Logic  \\n–––, 1984, , (Series: Modern European Philosophy), Cambridge:\\nCambridge University Press.  \\nHusserl and Realism in Logic and\\nMathematics  \\n–––, 1989, “Sense Perceptual Intuition,\\nMathematical Existence, and Logical Imagination”, , 4(2): 154–194.  \\nPhilosphia\\nMathematica  \\nTroelstra, A. S., 1986, “Note to and ”, in Gödel 1990, pp. 217–241.  \\n1958  \\n1972  \\nTroelstra, A. S. (ed.), 1973, , (Lecture Notes in\\nMathematics, Volume 344), Berlin: Springer-Verlag.  \\nMetamathematical Investigation\\nof Intuitionistic Arithmetic and Analysis  \\nTuring, A. M., 1937, “On Computable Numbers, with an\\nApplication to the Entscheidungsproblem”, (Series 2), 42: 230–265.  \\nProceedings of the\\nLondon Mathematical Society  \\nvan Atten, Mark, 2005, “On Gödel’s Awareness of\\nSkolem’s Helsinki Lecture”, , 26(4): 321–326.  \\nHistory and Philosophy of\\nLogic  \\n–––, 2006, “Two Draft Letters from\\nGödel on Self-knowledge of Reason”, , 14(2): 255–261.  \\nPhilosophia\\nMathematica  \\n–––, 2015, “Essays on Gödel’s\\nReception of Leibniz, Husserl and Brouwer”, Springer.  \\nvan Heijenoort, J. (ed.), 1967, , Cambridge, MA:\\nHarvard University Press.  \\nFrom Frege to Gödel: A\\nsourcebook in mathematical logic, 1879–1931  \\nvan Oosten, Jaap, 2008, (Studies in Logic and Foundations of\\nMathematics: Volume 152), Amsterdam: Elsevier.  \\nRealizability: An Introduction to its\\nCategorical Side  \\nvon Neumann, John, 2005, (History of Mathematics, Volume 27), foreword by P. Lax,\\nintroduction by Marina von Neumann Whitman, preface and introductory\\ncomments by Miklós Rédei (ed.), Providence, RI: American\\nMathematical Society.  \\nJohn von Neumann: Selected\\nLetters  \\nVäänänen, Jouko, 2014, “Multiverse Set Theory\\nand Absolutely Undecidable Propositions”, in , J. Kennedy (ed.), Cambridge: Cambridge University\\nPress, 2014.  \\nInterpreting\\nGödel  \\nWang, Hao, 1957, “The Axiomatization of Arithmetic”, , 22: 145–158.  \\nJournal of Symbolic Logic  \\n–––, 1973, , London: Routledge.  \\nFrom Mathematics to\\nPhilosophy  \\n–––, 1981, “Some Facts about Kurt\\nGödel”, , 46(3):\\n653–659.  \\nJournal of Symbolic Logic  \\n–––, 1987, , Cambridge, MA: MIT Press.  \\nReflections on Kurt\\nGödel  \\n–––, 1993, , New York: Dover Publications Inc., 2nd edition.  \\nPopular Lectures on Mathematical\\nLogic  \\n–––, 1996, (Representation and Mind), Cambridge,\\nMA: MIT Press.  \\nA Logical Lourney: From\\nGödel to Philosophy  \\nWeingartner, P., and L. Schmetterer (eds.), 1987, , (History of Logic,\\nNumber 4), Naples: Bibliopolis.  \\nGödel\\nRemembered: Salzburg 10–12 July 1983  \\nWilkie, Alex, and J.B. Paris, 1987, “On the scheme of\\ninduction for bounded arithmetic formulas”, 35:\\n261–302.  \\nWoodin, W. Hugh, 1988, “Supercompact Cardinals, Sets of\\nReals, and Weakly Homogeneous Trees”, , 85(18):\\n6587–6591.  \\nProceedings of the\\nNational Academy of Sciences of the U.S.A.  \\n–––, 2001a, “The Continuum Hypothesis.\\nI”, ,\\n48(6): 567–576.  \\nNotices of the American Mathematical Society  \\n–––, 2001b, “The Continuum Hypothesis.\\nII”, ,\\n48(7): 681–690.  \\nNotices of the American Mathematical Society  \\n–––, 2002, “Correction: ‘The\\nContinuum Hypothesis. II’”, , 49(1): 46.  \\nNotices of the American\\nMathematical Society  \\nYourgrau, Palle, 2005, , New York: Basic Books.  \\nA World Without Time. The Forgotten\\nLegacy of Gödel and Einstein  \\nZach, Richard, 1999, “Completeness Before Post: Bernays,\\nHilbert, and the Development of Propositional Logic”, , 5(3): 331–366.  \\nBulletin of Symbolic Logic  \\n–––, 2003, “Hilbert’s\\nProgram”, in (Fall Edition), Edward N. Zalta (ed.), URL =\\n < >.  \\nThe Stanford Encyclopedia of Philosophy  \\nhttps://plato.stanford.edu/archives/fall2003/entries/hilbert-program/'),\n",
       " Document(metadata={'Header 1': 'Kurt Gödel', 'Header 2': 'Academic Tools'}, page_content='Academic Tools'),\n",
       " Document(metadata={'Header 1': 'Kurt Gödel', 'Header 2': 'Academic Tools'}, page_content='.  \\nHow to cite this entry  \\nat the .  \\nPreview the PDF version of this entry  \\nFriends of the SEP Society  \\nat the Internet Philosophy Ontology Project (InPhO).  \\nLook up topics and thinkers related to this entry  \\nat , with links to its database.  \\nEnhanced bibliography for this entry  \\nPhilPapers  \\nOther Internet Resources  \\nAvigad, Jeremy,\\n “ ”,\\n manuscript in PDF available online.  \\nGödel and the metamathematical tradition  \\nKoellner, Peter,\\n “ ”,\\n manuscript in PDF available online.  \\nTruth in Mathematics:The question of Pluralism  \\n.  \\nThe Bernays Project  \\nRelated Entries  \\n| | | | | | | | | | | |  \\nGödel, Kurt: incompleteness theorems  \\nHilbert, David: program in the foundations of mathematics  \\nHusserl, Edmund  \\nLeibniz, Gottfried Wilhelm  \\nmathematics, philosophy of: intuitionism  \\nmathematics, philosophy of: Platonism  \\nmodel theory  \\nmodel theory: first-order  \\nphenomenology  \\nrealism  \\nset theory  \\nset theory: continuum hypothesis  \\nset theory: large cardinals and determinacy'),\n",
       " Document(metadata={'Header 1': 'Kurt Gödel', 'Header 2': 'Academic Tools', 'Header 3': 'Acknowledgments'}, page_content='Acknowledgments'),\n",
       " Document(metadata={}, page_content='This entry was very much improved by discussion and correspondence\\nwith the following: Aki Kanamori, who made helpful corrections and\\ncomments to section 2.4; Jouko Väänänen, whose\\nexpertise in all areas of mathematical logic the author benefited from\\nin a great many invaluable discussions regarding the material in\\nsection 2; my sub-editor Richard Zach, whose many important and\\nhelpful suggestions led to a vast improvement of this entry, and an\\nanonymous referee for helpful comments and corrections. The author is\\ngrateful to the NWO for their support during the last period of the\\nwriting of this entry, to the Institute for Advanced Study for their\\nhospitality during the writing of this entry, and to Marcia Tucker of\\nthe IAS and the Rare Books and Special Collections department of\\nFirestone Library for all of their assistance over the years .  \\nby < >  \\nCopyright © 2015  \\nJuliette Kennedy  \\njuliette kennedy helsinki fi  \\n.  \\n@  \\n.  \\nOpen access to the SEP is made possible by a world-wide funding initiative. The Encyclopedia Now Needs Your Support Please Read How You Can Help Keep the Encyclopedia Free  \\nEnd footer menu End mirrors End site credits'),\n",
       " Document(metadata={'Header 4': 'Browse'}, page_content='Browse'),\n",
       " Document(metadata={'Header 4': 'Browse'}, page_content=\"Table of Contents  \\nWhat's New  \\nRandom Entry  \\nChronological  \\nArchives\"),\n",
       " Document(metadata={'Header 4': 'About'}, page_content='About'),\n",
       " Document(metadata={'Header 4': 'About'}, page_content='Editorial Information  \\nAbout the SEP  \\nEditorial Board  \\nHow to Cite the SEP  \\nSpecial Characters  \\nAdvanced Tools  \\nAccessibility  \\nContact'),\n",
       " Document(metadata={'Header 4': 'Support SEP'}, page_content='Support SEP'),\n",
       " Document(metadata={'Header 4': 'Support SEP'}, page_content='Support the SEP  \\nPDFs for SEP Friends  \\nMake a Donation  \\nSEPIA for Libraries'),\n",
       " Document(metadata={'Header 4': 'Mirror Sites'}, page_content='Mirror Sites'),\n",
       " Document(metadata={}, page_content=\"View this site from another server:  \\nUSA (Main Site)  \\nPhilosophy, Stanford University  \\nInfo about mirror sites  \\nThe Stanford Encyclopedia of Philosophy is by , Department of Philosophy, Stanford University  \\ncopyright © 2023  \\nThe Metaphysics Research Lab  \\nLibrary of Congress Catalog Data: ISSN 1095-5054  \\n$('.dropdown-toggle').dropdown();\")]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://plato.stanford.edu/entries/goedel/\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "]\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text_from_url(url)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a1b76df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "json_data=requests.get(\"https://api.smith.langchain.com/openapi.json\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af9d8126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page': 1,\n",
       " 'pageSize': 10,\n",
       " 'total': 408,\n",
       " 'hasNext': True,\n",
       " 'items': [{'id': 1,\n",
       "   'createdAt': '2025-04-02T18:11:13.596Z',\n",
       "   'updatedAt': '2025-04-04T02:04:05.285Z',\n",
       "   'deletedAt': '0001-01-01T00:00:00Z',\n",
       "   'status': 'processed',\n",
       "   'type': '',\n",
       "   'command': '',\n",
       "   'catalogInstructionId': 2,\n",
       "   'catalogId': 89,\n",
       "   'filename': '',\n",
       "   'temporaryDownloadLink': '',\n",
       "   'email': '',\n",
       "   'params': None},\n",
       "  {'id': 2,\n",
       "   'createdAt': '2025-04-02T18:13:05.976Z',\n",
       "   'updatedAt': '2025-04-02T18:57:07.81Z',\n",
       "   'deletedAt': '0001-01-01T00:00:00Z',\n",
       "   'status': 'processed',\n",
       "   'type': '',\n",
       "   'command': '',\n",
       "   'catalogInstructionId': 2,\n",
       "   'catalogId': 89,\n",
       "   'filename': '1743620227_2_89_2.json',\n",
       "   'temporaryDownloadLink': 'https://storage.googleapis.com/ra-data-terminal-bucket-files-prd/1743620227_2_89_2.json?Expires=1743706627&GoogleAccessId=svc-ingest-bucket-prd%40ra-data-analytics.iam.gserviceaccount.com&Signature=Yqq2MfCZKktwj2qgqzp2yk5BwXdCfMYRuvu9cyc6%2FIOnwsztBncl33xAUSyoHvOw%2FwR8CGSLH7y26KCIcdyOCNDXrK1gnUvdH7s%2FgP469IkIw9%2BCue0mbtjZ07ckpsMnVRDy4IxnTl1c0q00ctgc96PoPLkRmZF2uUlLYAKatSI%2FQL4w69YIH6eX%2B3XF0k1f7C%2Bnof5oxXHWj5py0G8I1hAKagawDAjloHphTbFzy2YQpyWeNK044YJFt2BoWAiEI9tmkOdPsIYGH0cOmOpteG6S8ZSDePUaZJmzqRmsElBmRE6VnsoUchBt%2FIkZaGzWD72pkNBbI6Oc6i92MrMN8w%3D%3D',\n",
       "   'email': '',\n",
       "   'params': None},\n",
       "  {'id': 3,\n",
       "   'createdAt': '2025-04-02T18:59:05.488Z',\n",
       "   'updatedAt': '2025-04-02T18:59:12.058Z',\n",
       "   'deletedAt': '0001-01-01T00:00:00Z',\n",
       "   'status': 'processed',\n",
       "   'type': '',\n",
       "   'command': '',\n",
       "   'catalogInstructionId': 2,\n",
       "   'catalogId': 89,\n",
       "   'filename': '1743620351_3_89_2.json',\n",
       "   'temporaryDownloadLink': 'https://storage.googleapis.com/ra-data-terminal-bucket-files-prd/1743620351_3_89_2.json?Expires=1743706752&GoogleAccessId=svc-ingest-bucket-prd%40ra-data-analytics.iam.gserviceaccount.com&Signature=bt6wYxseVubpDmN0iCuRqAnGCPdCmJ8qu2qgH123clz0mZfSYLerR4s%2Bj5efGN0ed31gOx%2Boe32pFS9KNAqu5qpQ5%2FMkNHZvJvVHCN%2FVYugbFuf1PvfJ3zK0%2FnY8D%2BrsZxSFNdwg9zm2pqlVqrGs4c38L%2FlIHsP%2FBiqG3VATCsEywFxWBf4GdDbLRHthjmHn0FBdH4MbxWZcWENEK7H0lhVYX8duEt0dDem9GTWaP1nEXqtHcHIZcrP1h58Qp%2Fad%2Bc6guyZabo4DhZmttiTisx1yJaTotsv1Xhv0%2F1xbh9AMjSmVmqWDQNZTM%2Bo0uoIIWAFF8fq7%2FKWWrLiprTkJKw%3D%3D',\n",
       "   'email': '',\n",
       "   'params': None},\n",
       "  {'id': 4,\n",
       "   'createdAt': '2025-04-03T14:26:50.285Z',\n",
       "   'updatedAt': '2025-04-03T14:26:53.599Z',\n",
       "   'deletedAt': '0001-01-01T00:00:00Z',\n",
       "   'status': 'processed',\n",
       "   'type': '',\n",
       "   'command': '',\n",
       "   'catalogInstructionId': 3,\n",
       "   'catalogId': 90,\n",
       "   'filename': '1743690413_4_90_3.json',\n",
       "   'temporaryDownloadLink': 'https://storage.googleapis.com/ra-data-terminal-bucket-files-prd/1743690413_4_90_3.json?Expires=1743776813&GoogleAccessId=svc-ingest-bucket-prd%40ra-data-analytics.iam.gserviceaccount.com&Signature=H0hic25wgVzvSLTFW7sRguAQ1QPR37GiX8YQHOkw1y2UrRdV%2BZJDFS1hBppCygTvLdAL7e88VzvlkgRTyJ9fC9AbAnGCJ1ULEUp0DKKql04BtuMjX7mVAzgLMpbfswb38hJL4lehEULQT7lTgQeH%2BA%2BOTxQpRelz5nGcVDEOVVBWpR9A0Ea%2BEreFgMKCOLdXVXLaTJKtYuZpmPopUfDNQei3wO8R5KYXnQ9j%2F07hLN6JkxBevmGhmsXhbLUl7%2FQUTxIReabJd9%2FqvIerL2fwyIzCC9Hoywy7YdB6Y2BpPIhmnEIyDO%2FpVunGc%2FLR7wxZV%2BC8beGjtwUvou5TJpW8YQ%3D%3D',\n",
       "   'email': '',\n",
       "   'params': None},\n",
       "  {'id': 5,\n",
       "   'createdAt': '2025-04-03T14:52:11.023Z',\n",
       "   'updatedAt': '2025-04-03T14:52:12.37Z',\n",
       "   'deletedAt': '0001-01-01T00:00:00Z',\n",
       "   'status': 'processed',\n",
       "   'type': '',\n",
       "   'command': '',\n",
       "   'catalogInstructionId': 3,\n",
       "   'catalogId': 90,\n",
       "   'filename': '1743691932_5_90_3.json',\n",
       "   'temporaryDownloadLink': 'https://storage.googleapis.com/ra-data-terminal-bucket-files-prd/1743691932_5_90_3.json?Expires=1743778332&GoogleAccessId=svc-ingest-bucket-prd%40ra-data-analytics.iam.gserviceaccount.com&Signature=BiYi5G3qFZZnTVWSkF9RsfN82QNashaU7ll9qbOrD2brtlyB1hchoDxzjpViykwwA4XLwaeIj7YwZJWCzUNH6NYI%2BehEGlVfp62%2FSxfLVRbxJJe9sB35wtT7sEadu%2BbeQTSaMjSnWiZfVqIhVIdPHq8kXOD7nqDdap5XWxXCYph5W93%2B%2FguF4OfjjAdNpyZst%2Flt51032%2F39GibRhPwzJkyMKYuxMLNn4ZVERtbef70ePBB8Z84jBwfbSvc3Nnz40V22fAwryHIyIC8GNlGhVqN132fiHPG7mgGUVIyk5MUPc1W9oN2%2Fb2YNpR7FxwLn77q3YGqDrrQ6COGaiTluyg%3D%3D',\n",
       "   'email': '',\n",
       "   'params': None},\n",
       "  {'id': 6,\n",
       "   'createdAt': '2025-04-03T19:48:19.322Z',\n",
       "   'updatedAt': '2025-04-03T19:48:22.656Z',\n",
       "   'deletedAt': '0001-01-01T00:00:00Z',\n",
       "   'status': 'processed',\n",
       "   'type': '',\n",
       "   'command': '',\n",
       "   'catalogInstructionId': 3,\n",
       "   'catalogId': 90,\n",
       "   'filename': '1743709702_6_90_3.json',\n",
       "   'temporaryDownloadLink': 'https://storage.googleapis.com/ra-data-terminal-bucket-files-prd/1743709702_6_90_3.json?Expires=1743796102&GoogleAccessId=svc-ingest-bucket-prd%40ra-data-analytics.iam.gserviceaccount.com&Signature=Lb264ZWQ0J0d0gNVcgBrUbaNxwb9wkX2l9bUBSqeBCamR2fc0PWDc2pj%2FfjS9O2oZMnpbKZsHoJrm2YmDmJtSSlgM4YE1CbUc6pU0TpA3QfYDRJgBbAg9eGJR%2BOMzPoTMR6Z8i2YvZfhFLZaKYNwWR2m9LwBxC6bhpO6nyl0rGmsqA3EFAxeSsZ0rarcOSEEGK0UT3sLju0hpwY0eIv5QV38u5s9kAiS%2B6KIS2kwKlGrxbgZmAfMk%2B7H8LZ%2B89bEIc%2FhZ6PXzz4MGaPUfMoYHxwcv%2FfEiAuDq3fH9PBurO8585uTeHvZMMGfBIinUVJaGNUc2oWIa1iMwv1JLS6xjw%3D%3D',\n",
       "   'email': '',\n",
       "   'params': None},\n",
       "  {'id': 7,\n",
       "   'createdAt': '2025-04-03T19:50:01.19Z',\n",
       "   'updatedAt': '2025-04-03T19:50:02.098Z',\n",
       "   'deletedAt': '0001-01-01T00:00:00Z',\n",
       "   'status': 'processed',\n",
       "   'type': '',\n",
       "   'command': '',\n",
       "   'catalogInstructionId': 3,\n",
       "   'catalogId': 90,\n",
       "   'filename': '1743709801_7_90_3.json',\n",
       "   'temporaryDownloadLink': 'https://storage.googleapis.com/ra-data-terminal-bucket-files-prd/1743709801_7_90_3.json?Expires=1743796202&GoogleAccessId=svc-ingest-bucket-prd%40ra-data-analytics.iam.gserviceaccount.com&Signature=ZBFr2HGUPwxYy3WVYTVQ6JRnYNEyi9SRva3%2FI2dByYd9QqIsvMXVfysYy%2Fieoguj5nBSoYp1VPvthjCXm5tJzJPOGd2cSdbomJbo0S8Dkyc%2B2x1J0aIyZ3Av3d0HMyGo66YOyeZC%2BCe%2F9n2qVssEpvwHVBSnsvYADcIKT6TGiVGp7gkbnFyE2LujiKTyCFFHv6W5VTd4kzc%2FCDykpGboumLIBu9%2BiLTDe1LXCOA4UI2ecNZWiKh2vd4wmWfKFAPvTO6zGBjl1mI%2FZ5aGCkbQ1jWIcsmjqU8OvuD0Kvc3kXSRA2pqVC7zUxVf6M8sSYPaQyLXJR24rTZkyVPBn9lNMg%3D%3D',\n",
       "   'email': '',\n",
       "   'params': None},\n",
       "  {'id': 8,\n",
       "   'createdAt': '2025-04-03T19:56:27.742Z',\n",
       "   'updatedAt': '2025-04-03T19:56:28.954Z',\n",
       "   'deletedAt': '0001-01-01T00:00:00Z',\n",
       "   'status': 'processed',\n",
       "   'type': '',\n",
       "   'command': '',\n",
       "   'catalogInstructionId': 3,\n",
       "   'catalogId': 90,\n",
       "   'filename': '1743710188_8_90_3.json',\n",
       "   'temporaryDownloadLink': 'https://storage.googleapis.com/ra-data-terminal-bucket-files-prd/1743710188_8_90_3.json?Expires=1743796588&GoogleAccessId=svc-ingest-bucket-prd%40ra-data-analytics.iam.gserviceaccount.com&Signature=Y0ilpz32fm8RKu6ZR6YWY9cKrNt%2B8iessTcrB1dZBJ%2F7d0VppsXUbYOOaVW5WbUMsQ%2Bl%2BkI2g9BK%2BWu5iKuCb4DpUzRpsQtIInfVxgbcHCS21IvqtOPikWm6G9yfzbKj8OqK4tUZLGn58yr%2BbcEFvOERj7KBnQWpgcl2vpaiRCrIwTNvVuP4llXWdy6Sn%2BXi9pW3xzbuJI2iErliJSPyKri85G%2BW6Wc6F2pcxJgfporEa%2FcLSvSWbAiYaWiKmt69CnNSVVYCJisrfvDWFJyq%2FIYj9YG3ceaXHFqC8hQFnCjnceZ7cqozTZ15bVoyP37EK2XlmPW6KqCTwrh63W5teA%3D%3D',\n",
       "   'email': '',\n",
       "   'params': None},\n",
       "  {'id': 9,\n",
       "   'createdAt': '2025-04-03T19:58:36.848Z',\n",
       "   'updatedAt': '2025-04-03T19:58:37.326Z',\n",
       "   'deletedAt': '0001-01-01T00:00:00Z',\n",
       "   'status': 'processed',\n",
       "   'type': '',\n",
       "   'command': '',\n",
       "   'catalogInstructionId': 3,\n",
       "   'catalogId': 90,\n",
       "   'filename': '1743710317_9_90_3.json',\n",
       "   'temporaryDownloadLink': 'https://storage.googleapis.com/ra-data-terminal-bucket-files-prd/1743710317_9_90_3.json?Expires=1743796717&GoogleAccessId=svc-ingest-bucket-prd%40ra-data-analytics.iam.gserviceaccount.com&Signature=ikiaPUGqr1HKsgwWwHi%2BZkwhI7eGgKI9Xa0s9If%2FHLiFF%2B2DMeaIljuHnAgbphtKvNlGkhRnhqOoCTpZlrm%2Bg56C1G6uvVA9ek75xnDCKkAisBKAJT%2Bb0rPl%2BjHo4Q9G%2BbQ4q8Bbf0hVlR1bmQXQM7i8%2Bk2Z160vCT1qtYuYnDNlcyhDb%2FVIVDsWap5Jr4sretDPeezK55vcOIXG7cpG3o43UjWfqTQyceuCAuV6zZSfPfZzX2tciO7sk3U%2F1bc9AqaZHI4KdfE30T63jRGARQ0yMgtAat8HFUiqsvj1Ysh%2BcQT8lGMgajnn9gi3xT7qRQFa8EKPUXN%2B%2BYcrsqsufw%3D%3D',\n",
       "   'email': '',\n",
       "   'params': None},\n",
       "  {'id': 10,\n",
       "   'createdAt': '2025-04-03T20:02:03.192Z',\n",
       "   'updatedAt': '2025-04-03T20:02:04.058Z',\n",
       "   'deletedAt': '0001-01-01T00:00:00Z',\n",
       "   'status': 'processed',\n",
       "   'type': '',\n",
       "   'command': '',\n",
       "   'catalogInstructionId': 3,\n",
       "   'catalogId': 90,\n",
       "   'filename': '1743710523_10_90_3.json',\n",
       "   'temporaryDownloadLink': 'https://storage.googleapis.com/ra-data-terminal-bucket-files-prd/1743710523_10_90_3.json?Expires=1743796924&GoogleAccessId=svc-ingest-bucket-prd%40ra-data-analytics.iam.gserviceaccount.com&Signature=ZHBi1Uel19%2BGZAVYduMa9QHjsAJ2iD8ZA1floLj95BkHu0Mi3Y5rAR7p1NWnMbVC8zSiCKDeS3wvXyMnXjsnNoZnrvvsdecqkszITZqvrFyQA%2Fc%2FNbCcwui7coCkSZdnZnb9o3ykqNcKhMoGq1snkKwWc9SAZGJNIheetmcC0ype%2FfucX3m3bkQLfuY6RbgL8ZQC%2B2pTWiHi021NuQDDMp7ENkmQ41XjTls%2BPC02tP9rfOC%2BxaDVr83tC8pM%2B9Q%2B90LsHGcEIQcKMrNx9KpzeNvRyJ3eewYHKos%2BI76a1JvC%2F2XP4V0atatHWYkuleKz6UcMIbRW6v5OUYokPhoMKQ%3D%3D',\n",
       "   'email': '',\n",
       "   'params': None}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "json_data=requests.get(\"https://data-terminal.dataanalytics.obviobrasil.com.br/orders\").json()\n",
    "json_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
